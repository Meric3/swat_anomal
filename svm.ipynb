{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T05:45:55.049605Z",
     "start_time": "2019-04-16T05:45:53.253844Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mjh319/anaconda3/envs/latest_3.6/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import utils.dataset as dataset\n",
    "import utils.preprocessing as preprocessing\n",
    "from utils.logger import Logger\n",
    "import lstm.model as model\n",
    "import utils.postprocessing as postprocessing\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from numpy import linalg as LA\n",
    "import numpy as np\n",
    "import logging\n",
    "import argparse\n",
    "from sklearn import svm\n",
    "\n",
    "from astropy.convolution import Gaussian1DKernel, convolve\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T05:45:55.084875Z",
     "start_time": "2019-04-16T05:45:55.067847Z"
    }
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\"--cuda\", default = True, action = \"store_true\")\n",
    "parser.add_argument(\"--tf_log\", default = False, action = \"store_true\")\n",
    "parser.add_argument(\"--model_name\", type = str, default = \"enc_dec\")\n",
    "parser.add_argument(\"--batch_size\", type = int, default = 256)\n",
    "parser.add_argument(\"--clip\", type = int, default=1)\n",
    "\n",
    "parser.add_argument(\"--train_path\", type = str, default = \"../../DATA/SWaT/SWaT_Physical/SWaT_Dataset_Normal_v0.csv\")\n",
    "parser.add_argument(\"--test_path\", type = str, default = \"../../DATA/SWaT/SWaT_Physical/SWaT_Dataset_Attack_v0.csv\")\n",
    "parser.add_argument(\"--attack_list_path\", type = str, default = '../../DATA/SWaT/SWaT_Physical/attack_list.csv')\n",
    "\n",
    "parser.add_argument(\"--dropout\", type = float, default = 0.5)\n",
    "parser.add_argument(\"--hidden_size\", type = int, default = 4)\n",
    "parser.add_argument(\"--nlayers\", type = int, default = 2)\n",
    "parser.add_argument(\"--lr\", type = float, default = 0.0001)\n",
    "\n",
    "parser.add_argument(\"--cell_type\", type=str, default=\"LSTM\")\n",
    "parser.add_argument(\"--epoch\", type=int, default=10)\n",
    "parser.add_argument(\"--seq_length\", type=int, default=2)\n",
    "parser.add_argument('--selected_dim', nargs='+', type=int, default=[36, 38, 28, 40])\n",
    "args = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T05:45:55.118378Z",
     "start_time": "2019-04-16T05:45:55.101843Z"
    }
   },
   "outputs": [],
   "source": [
    "class SVM_solver():\n",
    "    def __init__(self, args):\n",
    "        \n",
    "        torch.manual_seed(777)\n",
    "        torch.cuda.manual_seed_all(777)\n",
    "        np.random.seed(777)\n",
    "        \n",
    "        self.attack_list = pd.read_csv(args.attack_list_path, error_bad_lines=False, sep='\\t')\n",
    "        train_x, test_x, self.test_y = dataset.svm_dataset(train_path = args.train_path, test_path = args.test_path)\n",
    "        self.train_x = train_x[:,args.selected_dim]\n",
    "        self.test_x = test_x[:, args.selected_dim]\n",
    "        \n",
    "        \n",
    "    def fit(self, load):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        nu = 0.001\n",
    "        gamma = 0.001\n",
    "        clf = svm.OneClassSVM(nu=nu, kernel=\"rbf\", gamma=gamma, verbose=False)\n",
    "        clf.fit(self.train_x)\n",
    "        end_time = time.time()\n",
    "        preds = clf.predict(self.test_x)         \n",
    "        end_time = time.time()\n",
    "\n",
    "        f1 = metrics.f1_score(self.test_y, preds, pos_label = -1)\n",
    "        print(f1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T05:45:55.294886Z",
     "start_time": "2019-04-16T05:45:55.274312Z"
    }
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\"--cuda\", default=True, action=\"store_true\")\n",
    "parser.add_argument(\"--tf_log\", default=False, action=\"store_true\")\n",
    "parser.add_argument(\"--model_name\", type=str, default=\"enc_dec\")\n",
    "parser.add_argument(\"--batch_size\", type=int, default=256)\n",
    "parser.add_argument(\"--clip\", type=int, default=1)\n",
    "\n",
    "parser.add_argument(\"--train_path\", type=str, default=\"../../DATA/SWaT/SWaT_Physical/SWaT_Dataset_Normal_v0.csv\")\n",
    "parser.add_argument(\"--test_path\", type=str, default=\"../../DATA/SWaT/SWaT_Physical/SWaT_Dataset_Attack_v0.csv\")\n",
    "parser.add_argument(\"--attack_list_path\", type=str, default='../../DATA/SWaT/SWaT_Physical/attack_list.csv')\n",
    "\n",
    "parser.add_argument(\"--dropout\", type=float, default=0.5)\n",
    "parser.add_argument(\"--hidden_size\", type=int, default=4)\n",
    "parser.add_argument(\"--nlayers\", type=int, default=2)\n",
    "parser.add_argument(\"--lr\", type=float, default=0.0001)\n",
    "\n",
    "parser.add_argument(\"--cell_type\", type=str, default=\"LSTM\")\n",
    "parser.add_argument(\"--epoch\", type=int, default=1)\n",
    "parser.add_argument(\"--seq_length\", type=int, default=2)\n",
    "parser.add_argument('--selected_dim', nargs='+', type=int, default=[36, 38, 28, 40])\n",
    "args = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T05:27:41.411703Z",
     "start_time": "2019-04-16T05:27:09.064698Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mjh319/workspace/swat_anomal/utils/dataset.py:113: FutureWarning: Currently, 'apply' passes the values as ndarrays to the applied function. In the future, this will change to passing it as Series objects. You need to specify 'raw=True' to keep the current behaviour, and you can pass 'raw=False' to silence this warning\n",
      "  \n",
      "/home/mjh319/workspace/swat_anomal/utils/dataset.py:114: FutureWarning: Currently, 'apply' passes the values as ndarrays to the applied function. In the future, this will change to passing it as Series objects. You need to specify 'raw=True' to keep the current behaviour, and you can pass 'raw=False' to silence this warning\n",
      "  pca.fit(X_train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(496800, 2) (449919, 2)\n",
      "(449919,) (449919,)\n",
      "0.035280447871553815\n"
     ]
    }
   ],
   "source": [
    "args.selected_dim = [35, 44]\n",
    "args.hidden_size = 4\n",
    "solver = SVM_solver(args = args)\n",
    "solver.fit(load = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T05:33:13.473632Z",
     "start_time": "2019-04-16T05:32:49.129225Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mjh319/workspace/swat_anomal/utils/dataset.py:114: FutureWarning: Currently, 'apply' passes the values as ndarrays to the applied function. In the future, this will change to passing it as Series objects. You need to specify 'raw=True' to keep the current behaviour, and you can pass 'raw=False' to silence this warning\n",
      "  pca.fit(X_train)\n",
      "/home/mjh319/workspace/swat_anomal/utils/dataset.py:115: FutureWarning: Currently, 'apply' passes the values as ndarrays to the applied function. In the future, this will change to passing it as Series objects. You need to specify 'raw=True' to keep the current behaviour, and you can pass 'raw=False' to silence this warning\n",
      "  X_train = pca.transform(X_train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(496800, 2) (449919, 2)\n",
      "(449919,) (449919,)\n",
      "0.8077569323301833\n"
     ]
    }
   ],
   "source": [
    "args.selected_dim = [35, 44]\n",
    "args.hidden_size = 4\n",
    "solver = SVM_solver(args = args)\n",
    "solver.fit(load = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T05:46:51.647726Z",
     "start_time": "2019-04-16T05:46:01.672091Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mjh319/workspace/swat_anomal/utils/dataset.py:114: FutureWarning: Currently, 'apply' passes the values as ndarrays to the applied function. In the future, this will change to passing it as Series objects. You need to specify 'raw=True' to keep the current behaviour, and you can pass 'raw=False' to silence this warning\n",
      "  y_train = y_train.rolling(10, min_periods=1).apply(check)\n",
      "/home/mjh319/workspace/swat_anomal/utils/dataset.py:115: FutureWarning: Currently, 'apply' passes the values as ndarrays to the applied function. In the future, this will change to passing it as Series objects. You need to specify 'raw=True' to keep the current behaviour, and you can pass 'raw=False' to silence this warning\n",
      "  y_test = y_test.rolling(10, min_periods=1).apply(check)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8077569323301833\n"
     ]
    }
   ],
   "source": [
    "args.selected_dim = [35, 44]\n",
    "args.hidden_size = 4\n",
    "solver = SVM_solver(args = args)\n",
    "solver.fit(load = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T05:51:51.980018Z",
     "start_time": "2019-04-16T05:48:26.411577Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mjh319/workspace/swat_anomal/utils/dataset.py:114: FutureWarning: Currently, 'apply' passes the values as ndarrays to the applied function. In the future, this will change to passing it as Series objects. You need to specify 'raw=True' to keep the current behaviour, and you can pass 'raw=False' to silence this warning\n",
      "  y_train = y_train.rolling(10, min_periods=1).apply(check)\n",
      "/home/mjh319/workspace/swat_anomal/utils/dataset.py:115: FutureWarning: Currently, 'apply' passes the values as ndarrays to the applied function. In the future, this will change to passing it as Series objects. You need to specify 'raw=True' to keep the current behaviour, and you can pass 'raw=False' to silence this warning\n",
      "  y_test = y_test.rolling(10, min_periods=1).apply(check)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3524330154683402\n"
     ]
    }
   ],
   "source": [
    "args.selected_dim = [0, 39]\n",
    "args.hidden_size = 4\n",
    "solver = SVM_solver(args = args)\n",
    "solver.fit(load = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.selected_dim = [0, 2]\n",
    "args.hidden_size = 4\n",
    "solver = SVM_solver(args = args)\n",
    "solver.fit(load = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-10T05:04:33.683604Z",
     "start_time": "2019-03-10T05:04:33.654325Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class ENCODER(nn.Module):\n",
    "\n",
    "    def __init__(self,args):\n",
    "        super(ENCODER, self).__init__()\n",
    "        self.args = args\n",
    "        self.drop = nn.Dropout(args['dropout'])\n",
    "        self.linear = nn.Linear(args['hidden_size'], args['data_dim'])\n",
    "\n",
    "        if args['cell_type'] in ['LSTM', 'GRU']:\n",
    "            self.rnn = getattr(nn, args['cell_type'])(args['rnn_inp_size'], args['hidden_size'], args['nlayers'], dropout=args['dropout'])\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
    "        self.linear.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, input, hidden, return_hiddens=False, noise=False):\n",
    "        output, hidden = self.rnn(input, hidden)\n",
    "        output = self.linear(output.contiguous().view(-1,self.args['hidden_size']))\n",
    "        output = output.contiguous().view(input.size()[0], -1, self.args['rnn_inp_size'])\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters()).data ############# 이게 무엇\n",
    "        if self.args['cell_type'] == 'LSTM':\n",
    "            return (Variable(weight.new(self.args['nlayers'], bsz, self.args['hidden_size']).zero_()),\n",
    "                    Variable(weight.new(self.args['nlayers'], bsz, self.args['hidden_size']).zero_()))\n",
    "\n",
    "    def repackage_hidden(self,h):\n",
    "        \"\"\"Wraps hidden states in new Variables, to detach them from their history.\"\"\"\n",
    "        if type(h) == tuple:\n",
    "            return tuple(self.repackage_hidden(v) for v in h)\n",
    "        else:\n",
    "            return Variable(h.data)\n",
    "\n",
    "    def extract_hidden(self, hidden):\n",
    "        if self.args['cell_type'] == 'LSTM':\n",
    "            return hidden[0][-1].data.cpu()  # hidden state last layer (hidden[1] is cell state)\n",
    "        else:\n",
    "            return hidden[-1].data.cpu()  # last layer\n",
    "\n",
    "        \n",
    "class DECODER(nn.Module):\n",
    "\n",
    "    def __init__(self,args):\n",
    "        super(DECODER, self).__init__()\n",
    "        self.args = args\n",
    "        self.drop = nn.Dropout(args['dropout'])\n",
    "        self.linear = nn.Linear(args['hidden_size'], args['data_dim'])\n",
    "\n",
    "        if args['cell_type'] in ['LSTM', 'GRU']:\n",
    "            self.rnn = getattr(nn, args['cell_type'])(args['rnn_inp_size'], args['hidden_size'], args['nlayers'], dropout=args['dropout'])\n",
    "\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
    "        self.linear.bias.data.fill_(0)\n",
    "        \n",
    "    def forward(self, input, hidden, return_hiddens=False, noise=False):\n",
    "        \n",
    "        output, hidden = self.rnn(input, hidden)\n",
    "        output = self.linear(output.contiguous().view(-1,self.args['hidden_size']))\n",
    "        output = output.contiguous().view(input.size()[0], -1, self.args['rnn_inp_size'])\n",
    "\n",
    "        return output, hidden\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters()).data ############# 이게 무엇\n",
    "        if self.args['cell_type'] == 'LSTM':\n",
    "            return (Variable(weight.new(self.args['nlayers'], bsz, self.args['hidden_size']).zero_()),\n",
    "                    Variable(weight.new(self.args['nlayers'], bsz, self.args['hidden_size']).zero_()))\n",
    "\n",
    "    def repackage_hidden(self,h):\n",
    "        \"\"\"Wraps hidden states in new Variables, to detach them from their history.\"\"\"\n",
    "        if type(h) == tuple:\n",
    "            return tuple(self.repackage_hidden(v) for v in h)\n",
    "        else:\n",
    "            return Variable(h.data)\n",
    "\n",
    "    def extract_hidden(self, hidden):\n",
    "        if self.args['cell_type'] == 'LSTM':\n",
    "            return hidden[0][-1].data.cpu()  # hidden state last layer (hidden[1] is cell state)\n",
    "        else:\n",
    "            return hidden[-1].data.cpu()  # last layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T02:08:26.400522Z",
     "start_time": "2019-04-11T02:08:26.357637Z"
    },
    "code_folding": [
     44,
     143
    ]
   },
   "outputs": [],
   "source": [
    "class Solver():\n",
    "    def __init__(self, args):\n",
    "        \n",
    "        torch.manual_seed(777)\n",
    "        torch.cuda.manual_seed_all(777)\n",
    "        np.random.seed(777)\n",
    "        \n",
    "        self.attack_list = pd.read_csv(args.attack_list_path, error_bad_lines=False, sep='\\t')\n",
    "\n",
    "        self.tf_log = args.tf_log\n",
    "        \n",
    "#         train_x, test_x, test_y = dataset.dataset(train_path = args.train_path, test_path = args.test_path)\n",
    "        train_x, test_x, test_y = dataset.lstm_dataset(train_path = args.train_path, test_path = args.test_path)\n",
    "        \n",
    "        train_x_batchfy = preprocessing.batchify(args, train_x, args.batch_size)\n",
    "        test_x_batchfy = preprocessing.batchify(args, test_x, args.batch_size)\n",
    "        generate_batchfy = preprocessing.batchify(args, test_x, 1)\n",
    "        train_generate_batchfy = preprocessing.batchify(args, train_x, 1)\n",
    "        \n",
    "        self.train_x_batchfy = train_x_batchfy[:,:,args.selected_dim]\n",
    "        self.test_x_batchfy = test_x_batchfy[:,:,args.selected_dim]\n",
    "        self.generate_batchfy = generate_batchfy[:,:,args.selected_dim]\n",
    "        self.train_generate_batchfy = train_generate_batchfy[:,:,args.selected_dim]\n",
    "        self.test_y = test_y\n",
    "        \n",
    "\n",
    "        self.args = args\n",
    "        self.encoder = model.ENCODER(self.args)\n",
    "        self.encoder.cuda()\n",
    "\n",
    "        self.decoder = model.DECODER(self.args)\n",
    "        self.decoder.cuda()\n",
    "\n",
    "        self.optim_enc   = torch.optim.Adam(self.encoder.parameters(), self.args.lr)\n",
    "        self.optim_dec   = torch.optim.Adam(self.decoder.parameters(), self.args.lr)\n",
    "\n",
    "        self.loss_fn = nn.MSELoss()    \n",
    "    \n",
    "        self.logger = Logger('./tf_logs')\n",
    "    \n",
    "        self.base_dir = Path('model_save')\n",
    "        self.base_dir.mkdir(parents=True,exist_ok=True)      \n",
    "\n",
    "\n",
    "    def load(self, path):\n",
    "        try:\n",
    "            print(\"=> loaded checkpoint\")\n",
    "        except:\n",
    "            print(\"=> Not exist checkpoint\")\n",
    "            pass        \n",
    "\n",
    "    def fit(self, load):\n",
    "        total_loss = 0\n",
    "        max_f1 = 0\n",
    "        total_length = self.train_x_batchfy.size(0) - 1\n",
    "        start_time = time.time()\n",
    "        \n",
    "        \n",
    "                                          \n",
    "        for epoch in range(0, self.args.epoch):\n",
    "\n",
    "            self.encoder.train()\n",
    "            self.decoder.train()\n",
    "                \n",
    "            hidden_enc = self.encoder.init_hidden(self.args.batch_size)\n",
    "\n",
    "            for batch, i in enumerate(range(0, self.train_x_batchfy.size(0) - 1, self.args.seq_length)):\n",
    "                outSeq = []\n",
    "                inputSeq, targetSeq = preprocessing.get_batch(self.args, self.train_x_batchfy, i)\n",
    "\n",
    "                if args.seq_length != targetSeq.size()[0] :\n",
    "                    continue\n",
    "                hidden_enc = self.encoder.repackage_hidden(hidden_enc)\n",
    "                self.optim_enc.zero_grad()\n",
    "                self.optim_dec.zero_grad()\n",
    "                \n",
    "                Outputseq_enc, hidden_enc = self.encoder.forward(inputSeq, hidden_enc, return_hiddens=True)\n",
    "                deccoder_input = Variable(torch.zeros(Outputseq_enc.size())).cuda()\n",
    "                \n",
    "                deccoder_input[0,:,:] = Outputseq_enc[-1,:,:] # inputSeq[-1,:,:]\n",
    "                deccoder_input[1:,:,:] = targetSeq[:-1,:,:]\n",
    "                \n",
    "                loss_enc = self.loss_fn(Outputseq_enc[-1,:,:].view(self.args.batch_size, -1), targetSeq[0,:,:].contiguous().view(self.args.batch_size, -1))\n",
    "                loss_enc.backward(retain_graph=True)\n",
    "                \n",
    "                \n",
    "                encoder_norm = sum(p.grad.data.abs().sum() for p in self.encoder.parameters())\n",
    "                \n",
    "                torch.nn.utils.clip_grad_norm_(self.encoder.parameters(), self.args.clip)\n",
    "                \n",
    "                self.optim_enc.step()     \n",
    "                \n",
    "                Outputseq_enc, hidden_enc = self.decoder.forward(deccoder_input, hidden_enc, return_hiddens=True)\n",
    "                loss_dec = self.loss_fn(Outputseq_enc.view(args.batch_size, -1), targetSeq.contiguous().view(args.batch_size, -1))   \n",
    "                loss_dec.backward()\n",
    "                \n",
    "                edecoder_norm = sum(p.grad.data.abs().sum() for p in self.decoder.parameters())\n",
    "                \n",
    "                \n",
    "                torch.nn.utils.clip_grad_norm_(self.decoder.parameters(), self.args.clip)\n",
    "                self.optim_dec.step()\n",
    "                \n",
    "                \n",
    "                \n",
    "                total_loss += loss_enc.item() + loss_dec.item()        \n",
    "\n",
    "                if batch % 30 == 0 and self.tf_log == True :\n",
    "                    print(encoder_norm)\n",
    "                    print(decoder_norm)\n",
    "                    # 1. Log scalar values (scalar summary)\n",
    "                    info = { 'enc_loss': loss_enc.item(), 'dec_loss' : loss_dec.item() }\n",
    "\n",
    "                    for tag, value in info.items():\n",
    "                        self.logger.scalar_summary(tag, value, epoch*total_length + i +1)\n",
    "\n",
    "                    # 2. Log values and gradients of the parameters (histogram summary)\n",
    "                    for tag, value in self.encoder.named_parameters():\n",
    "                        tag = tag.replace('.', '/')\n",
    "                        self.logger.histo_summary(tag, value.data.cpu().numpy(), epoch*total_length + i +1)\n",
    "                        \n",
    "                    for tag, value in self.decoder.named_parameters():\n",
    "                        tag = tag.replace('.', '/')\n",
    "                        self.logger.histo_summary(tag, value.data.cpu().numpy(), epoch*total_length + i +1)\n",
    "            \n",
    "            total_loss = 0    \n",
    "            if len(self.args.selected_dim) == 1:\n",
    "                self.anomal_score = postprocessing.get_anomalscore_encdec_1dim(base_model = self,  \\\n",
    "                                                                 generate_batchfy = self.generate_batchfy, length = 449916,args = self.args)\n",
    "            else:    \n",
    "                self.anomal_score = postprocessing.get_anomalscore_encdec(base_model = self,  \\\n",
    "                                                                 generate_batchfy = self.generate_batchfy, length = 449916,args = self.args)\n",
    "\n",
    "            self.anomal_score = LA.norm(self.anomal_score, axis=1)\n",
    "    #TODO conv 설정\n",
    "            max_conv, max_pre, max_recall, max_f1_tp, max_zerolist, find_attack_list = postprocessing.evaluate_conv(self.anomal_score, self.test_y, self.attack_list, 400)\n",
    "    \n",
    "          \n",
    "            if max_f1_tp > max_f1:\n",
    "                end_time = time.time()\n",
    "                \n",
    "                print(\"epoch[{}]\\t conv[{}]\\t precision[{}]\\t recall[{}]\\t f1[{}]\\t findnum[{}]\\t time[{}]\".format(epoch, max_conv, max_pre, max_recall, max_f1_tp,36 - max_zerolist, end_time - start_time))\n",
    "                \n",
    " \n",
    "    def save_checkpoint(self, args, state):\n",
    "        checkpoint = Path(self.base_dir, str(args.selected_dim))\n",
    "        checkpoint = checkpoint.with_suffix('.pth')\n",
    "        torch.save(state, checkpoint)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T02:02:27.285928Z",
     "start_time": "2019-04-11T02:02:27.256115Z"
    }
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\"--cuda\", default=True, action=\"store_true\")\n",
    "parser.add_argument(\"--tf_log\", default=False, action=\"store_true\")\n",
    "parser.add_argument(\"--model_name\", type=str, default=\"enc_dec\")\n",
    "parser.add_argument(\"--batch_size\", type=int, default=256)\n",
    "parser.add_argument(\"--clip\", type=int, default=1)\n",
    "\n",
    "parser.add_argument(\"--train_path\", type=str, default=\"../../DATA/SWaT/SWaT_Physical/SWaT_Dataset_Normal_v0.csv\")\n",
    "parser.add_argument(\"--test_path\", type=str, default=\"../../DATA/SWaT/SWaT_Physical/SWaT_Dataset_Attack_v0.csv\")\n",
    "parser.add_argument(\"--attack_list_path\", type=str, default='../../DATA/SWaT/SWaT_Physical/attack_list.csv')\n",
    "\n",
    "parser.add_argument(\"--dropout\", type=float, default=0.5)\n",
    "parser.add_argument(\"--hidden_size\", type=int, default=4)\n",
    "parser.add_argument(\"--nlayers\", type=int, default=2)\n",
    "parser.add_argument(\"--lr\", type=float, default=0.0001)\n",
    "\n",
    "parser.add_argument(\"--cell_type\", type=str, default=\"LSTM\")\n",
    "parser.add_argument(\"--epoch\", type=int, default=1)\n",
    "parser.add_argument(\"--seq_length\", type=int, default=2)\n",
    "parser.add_argument('--selected_dim', nargs='+', type=int, default=[36, 38, 28, 40])\n",
    "args = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T02:07:27.703164Z",
     "start_time": "2019-04-11T02:02:31.770447Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In train : {'Normal': 496800}\n",
      "In test : {'Attack': 54621, 'Normal': 395298}\n",
      "data length is 51\n",
      "epoch[0]\t conv[20]\t precision[0.97355307480304]\t recall[0.6402482561652112]\t f1[0.7724812794062424]\t findnum[11]\t time[287.5399081707001]\n"
     ]
    }
   ],
   "source": [
    "#일반 dataset 함수\n",
    "args.selected_dim = [0, 40]\n",
    "args.hidden_size = 4\n",
    "solver = Solver(args = args)\n",
    "solver.fit(load = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T02:14:39.112317Z",
     "start_time": "2019-04-11T02:09:45.104617Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[0]\t conv[20]\t precision[0.97355307480304]\t recall[0.6402482561652112]\t f1[0.7724812794062424]\t findnum[11]\t time[288.06569743156433]\n"
     ]
    }
   ],
   "source": [
    "#lstm dataset 함수\n",
    "args.selected_dim = [0, 40]\n",
    "args.hidden_size = 4\n",
    "solver = Solver(args = args)\n",
    "solver.fit(load = False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
