{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T06:24:17.700777Z",
     "start_time": "2019-03-06T06:24:17.676957Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import utils.dataset as dataset\n",
    "import utils.preprocessing as preprocessing\n",
    "from utils.logger import Logger\n",
    "import lstm.model as model\n",
    "import utils.postprocessing as postprocessing\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from numpy import linalg as LA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T23:48:32.478380Z",
     "start_time": "2019-03-03T23:48:32.450733Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T23:49:07.957054Z",
     "start_time": "2019-03-03T23:49:07.932733Z"
    }
   },
   "outputs": [],
   "source": [
    "train_path= '../../DATA/SWaT/SWaT_Physical/SWaT_Dataset_Normal_v0.csv'\n",
    "test_path = '../../DATA/SWaT/SWaT_Physical/SWaT_Dataset_Attack_v0.csv'\n",
    "\n",
    "train_x, test_x, test_y = dataset.pca_return(train_path = train_path, test_path = test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T06:02:06.675695Z",
     "start_time": "2019-02-28T06:02:06.658312Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T06:35:42.764950Z",
     "start_time": "2019-02-28T06:35:42.717765Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]predict 빈도: {-1: 4842, 1: 5158}\n"
     ]
    }
   ],
   "source": [
    "# comp_num = 45\n",
    "clf = svm.OneClassSVM(nu=0.001, kernel=\"rbf\", gamma=0.0001, verbose=True)\n",
    "clf.fit(train_x[0:20000,0:2])\n",
    "preds = clf.predict(test_x[5000:15000, 0:2]) \n",
    "unique, counts = np.unique(preds, return_counts=True)\n",
    "print('predict 빈도:', dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T06:10:27.161686Z",
     "start_time": "2019-02-28T06:10:27.112893Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]predict 빈도: {-1: 2390, 1: 7610}\n"
     ]
    }
   ],
   "source": [
    "# comp_num = 45\n",
    "clf = svm.OneClassSVM(nu=0.001, kernel=\"rbf\", gamma=0.0001, verbose=True)\n",
    "clf.fit(train_x[0:20000,0:4])\n",
    "preds = clf.predict(test_x[5000:15000, 0:4]) \n",
    "unique, counts = np.unique(preds, return_counts=True)\n",
    "print('predict 빈도:', dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T06:10:00.668563Z",
     "start_time": "2019-02-28T06:10:00.628819Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]predict 빈도: {-1: 4842, 1: 5158}\n"
     ]
    }
   ],
   "source": [
    "# comp_num = 2\n",
    "clf = svm.OneClassSVM(nu=0.001, kernel=\"rbf\", gamma=0.0001, verbose=True)\n",
    "clf.fit(train_x[0:20000,:])\n",
    "preds = clf.predict(test_x[5000:15000, :]) \n",
    "unique, counts = np.unique(preds, return_counts=True)\n",
    "print('predict 빈도:', dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T23:49:25.065766Z",
     "start_time": "2019-03-03T23:49:25.032434Z"
    }
   },
   "outputs": [],
   "source": [
    "class ENCODER(nn.Module):\n",
    "\n",
    "    def __init__(self,args):\n",
    "        super(ENCODER, self).__init__()\n",
    "        self.args = args\n",
    "        self.drop = nn.Dropout(args['dropout'])\n",
    "        self.linear = nn.Linear(args['hidden_size'], args['data_dim'])\n",
    "\n",
    "        if args['cell_type'] in ['LSTM', 'GRU']:\n",
    "            self.rnn = getattr(nn, args['cell_type'])(args['rnn_inp_size'], args['hidden_size'], args['nlayers'], dropout=args['dropout'])\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
    "        self.linear.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, input, hidden, return_hiddens=False, noise=False):\n",
    "        output, hidden = self.rnn(input, hidden)\n",
    "        output = self.linear(output.contiguous().view(-1,self.args['hidden_size']))\n",
    "        output = output.contiguous().view(input.size()[0], -1, self.args['rnn_inp_size'])\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters()).data ############# 이게 무엇\n",
    "        if self.args['cell_type'] == 'LSTM':\n",
    "            return (Variable(weight.new(self.args['nlayers'], bsz, self.args['hidden_size']).zero_()),\n",
    "                    Variable(weight.new(self.args['nlayers'], bsz, self.args['hidden_size']).zero_()))\n",
    "\n",
    "    def repackage_hidden(self,h):\n",
    "        \"\"\"Wraps hidden states in new Variables, to detach them from their history.\"\"\"\n",
    "        if type(h) == tuple:\n",
    "            return tuple(self.repackage_hidden(v) for v in h)\n",
    "        else:\n",
    "            return Variable(h.data)\n",
    "\n",
    "    def extract_hidden(self, hidden):\n",
    "        if self.args['cell_type'] == 'LSTM':\n",
    "            return hidden[0][-1].data.cpu()  # hidden state last layer (hidden[1] is cell state)\n",
    "        else:\n",
    "            return hidden[-1].data.cpu()  # last layer\n",
    "\n",
    "        \n",
    "class DECODER(nn.Module):\n",
    "\n",
    "    def __init__(self,args):\n",
    "        super(DECODER, self).__init__()\n",
    "        self.args = args\n",
    "        self.drop = nn.Dropout(args['dropout'])\n",
    "        self.linear = nn.Linear(args['hidden_size'], args['data_dim'])\n",
    "\n",
    "        if args['cell_type'] in ['LSTM', 'GRU']:\n",
    "            self.rnn = getattr(nn, args['cell_type'])(args['rnn_inp_size'], args['hidden_size'], args['nlayers'], dropout=args['dropout'])\n",
    "\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
    "        self.linear.bias.data.fill_(0)\n",
    "        \n",
    "    def forward(self, input, hidden, return_hiddens=False, noise=False):\n",
    "        \n",
    "        output, hidden = self.rnn(input, hidden)\n",
    "        output = self.linear(output.contiguous().view(-1,self.args['hidden_size']))\n",
    "        output = output.contiguous().view(input.size()[0], -1, self.args['rnn_inp_size'])\n",
    "\n",
    "        return output, hidden\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters()).data ############# 이게 무엇\n",
    "        if self.args['cell_type'] == 'LSTM':\n",
    "            return (Variable(weight.new(self.args['nlayers'], bsz, self.args['hidden_size']).zero_()),\n",
    "                    Variable(weight.new(self.args['nlayers'], bsz, self.args['hidden_size']).zero_()))\n",
    "\n",
    "    def repackage_hidden(self,h):\n",
    "        \"\"\"Wraps hidden states in new Variables, to detach them from their history.\"\"\"\n",
    "        if type(h) == tuple:\n",
    "            return tuple(self.repackage_hidden(v) for v in h)\n",
    "        else:\n",
    "            return Variable(h.data)\n",
    "\n",
    "    def extract_hidden(self, hidden):\n",
    "        if self.args['cell_type'] == 'LSTM':\n",
    "            return hidden[0][-1].data.cpu()  # hidden state last layer (hidden[1] is cell state)\n",
    "        else:\n",
    "            return hidden[-1].data.cpu()  # last layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T06:33:40.939910Z",
     "start_time": "2019-03-06T06:33:40.907246Z"
    }
   },
   "outputs": [],
   "source": [
    "class Solver():\n",
    "    def __init__(self, args):\n",
    "        \n",
    "        train_x, test_x, test_y = dataset.pca_return(train_path = args['train_path'], test_path = args['test_path'])\n",
    "        \n",
    "        train_x_batchfy = preprocessing.batchify(args, train_x, args['batch_size'])\n",
    "        test_x_batchfy = preprocessing.batchify(args, test_x, args['batch_size'])\n",
    "        generate_batchfy = preprocessing.batchify(args, test_x, 1)\n",
    "        train_generate_batchfy = preprocessing.batchify(args, train_x, 1)\n",
    "        \n",
    "        self.train_x_batchfy = train_x_batchfy[:,:,args['selected_dim']]\n",
    "        self.test_x_batchfy = test_x_batchfy[:,:,args['selected_dim']]\n",
    "        self.generate_batchfy = generate_batchfy[:,:,args['selected_dim']]\n",
    "        self.train_generate_batchfy = train_generate_batchfy[:,:,args['selected_dim']]\n",
    "        self.test_y = test_y\n",
    "        \n",
    "\n",
    "        self.args = args\n",
    "        self.encoder = model.ENCODER(self.args)\n",
    "        self.encoder.cuda()\n",
    "\n",
    "        self.decoder = model.DECODER(self.args)\n",
    "        self.decoder.cuda()\n",
    "\n",
    "        self.optim_enc   = torch.optim.Adam(self.encoder.parameters(), self.args['lr'])\n",
    "        self.optim_dec   = torch.optim.Adam(self.decoder.parameters(), self.args['lr'])\n",
    "\n",
    "        self.loss_fn = nn.MSELoss()    \n",
    "    \n",
    "        self.logger = Logger('./tf_logs')\n",
    "    \n",
    "        self.base_dir = Path('save',self.make_dir_name(args))\n",
    "        self.base_dir.mkdir(parents=True,exist_ok=True)      \n",
    "        \n",
    "        self.evaluate = args['evaluate']\n",
    "    \n",
    "    def make_dir_name(self, args):\n",
    "        return 'modelName:'+args['model_name']+'__cellType:'+args['cell_type'] \\\n",
    "                + '__hidSize:' + str(args['hidden_size']) + '__dropout:' + str(args['dropout'])\n",
    "\n",
    "    def load(self, path):\n",
    "        try:\n",
    "            checkpoint = torch.load(Path(path))\n",
    "            start_epoch = checkpoint['epoch']\n",
    "            self.encoder.load_state_dict(checkpoint['state_dict_enc'])\n",
    "            self.optim_enc.load_state_dict((checkpoint['optimizer_enc']))\n",
    "            self.decoder.load_state_dict(checkpoint['state_dict_dec'])\n",
    "            self.optim_dec.load_state_dict((checkpoint['optimizer_dec']))            \n",
    "            del checkpoint\n",
    "            print(\"=> loaded checkpoint\")\n",
    "        except:\n",
    "            print(\"=> Not exist checkpoint\")\n",
    "            pass        \n",
    "\n",
    "    def fit(self, load):\n",
    "        if load == True:\n",
    "            try:\n",
    "                checkpoint = torch.load(Path(self.base_dir,'checkpoint').with_suffix('.pth'))\n",
    "                start_epoch = checkpoint['epoch']\n",
    "                self.encoder.load_state_dict(checkpoint['state_dict_enc'])\n",
    "                self.optim_enc.load_state_dict((checkpoint['optimizer_enc']))\n",
    "                self.decoder.load_state_dict(checkpoint['state_dict_dec'])\n",
    "                self.optim_dec.load_state_dict((checkpoint['optimizer_dec']))            \n",
    "                del checkpoint\n",
    "                print(\"=> loaded checkpoint\")\n",
    "            except:\n",
    "                print(\"=> Not exist checkpoint\")\n",
    "                pass\n",
    "\n",
    "        total_loss = 0\n",
    "\n",
    "        for epoch in range(0, self.args['epoch']):\n",
    "\n",
    "            self.encoder.train()\n",
    "            self.decoder.train()\n",
    "            \n",
    "            print(\"post in\")\n",
    "            self.anomal_score = postprocessing.get_anomalscore_minmax(base_model = self,  \\\n",
    "                                                                 generate_batchfy = self.generate_batchfy, length = 449916,args = self.args)\n",
    "            print(anomal_score)\n",
    "            self.anomal_score = LA.norm(self.anomal_score, axis=1)\n",
    "            print(anomal_score)\n",
    "            precision, recall, thresholds = metrics.precision_recall_curve(  self.test_y[:449916].values.astype(int), self.anomal_score,pos_label =1)\n",
    "            beta = 1\n",
    "            f1 = (1+beta**2)*(precision*recall)/((beta**2*precision)+recall)\n",
    "            f1 = np.nan_to_num(f1)\n",
    "            max_f1 = f1[np.argmax(f1)]\n",
    "            print(\"precision : {}, recall : {}, f1 : {}\".format(precision,recall, f1))\n",
    "            break\n",
    "                \n",
    "            hidden_enc = self.encoder.init_hidden(self.args['batch_size'])\n",
    "\n",
    "            for batch, i in enumerate(range(0, self.train_x_batchfy.size(0) - 1, self.args['seq_length'])):\n",
    "                outSeq = []\n",
    "                inputSeq, targetSeq = preprocessing.get_batch(self.args, self.train_x_batchfy, i)\n",
    "\n",
    "                if args['seq_length'] != targetSeq.size()[0] :\n",
    "                    continue\n",
    "                hidden_enc = self.encoder.repackage_hidden(hidden_enc)\n",
    "                self.optim_enc.zero_grad()\n",
    "                self.optim_dec.zero_grad()\n",
    "                \n",
    "                Outputseq_enc, hidden_enc = self.encoder.forward(inputSeq, hidden_enc, return_hiddens=True)\n",
    "                deccoder_input = Variable(torch.zeros(Outputseq_enc.size())).cuda()\n",
    "                \n",
    "                deccoder_input[0,:,:] = Outputseq_enc[-1,:,:] # inputSeq[-1,:,:]\n",
    "                deccoder_input[1:,:,:] = targetSeq[:-1,:,:]\n",
    "                \n",
    "                loss_enc = self.loss_fn(Outputseq_enc[-1,:,:].view(self.args['batch_size'], -1), targetSeq[0,:,:].contiguous().view(self.args['batch_size'], -1))\n",
    "                loss_enc.backward(retain_graph=True)\n",
    "                \n",
    "                total_norm_1 = max(p.grad.data.abs().max() for p in self.encoder.parameters())\n",
    "                \n",
    "                torch.nn.utils.clip_grad_norm(self.encoder.parameters(), 0.1)\n",
    "                \n",
    "                total_norm_2 = max(p.grad.data.abs().max() for p in self.encoder.parameters())\n",
    "#                 break\n",
    "                \n",
    "#                 print(\"parameter norm {}, after clip {}\".format(total_norm_1, total_norm_2))\n",
    "#                 print(batch, self.train_x_batchfy.size(0) - 1)\n",
    "#                 if batch > 10:\n",
    "#                     break\n",
    "                \n",
    "                self.optim_enc.step()     \n",
    "                \n",
    "                Outputseq_enc, hidden_enc = self.decoder.forward(deccoder_input, hidden_enc, return_hiddens=True)\n",
    "                loss_dec = self.loss_fn(Outputseq_enc.view(args['batch_size'], -1), targetSeq.contiguous().view(args['batch_size'], -1))   \n",
    "                loss_dec.backward()\n",
    "                torch.nn.utils.clip_grad_norm(self.decoder.parameters(), self.args['clip'])\n",
    "                self.optim_dec.step()\n",
    "                \n",
    "                total_loss += loss_enc.item() + loss_dec.item()        \n",
    "\n",
    "                if batch % 10 == 0 :\n",
    "                    # 1. Log scalar values (scalar summary)\n",
    "                    info = { 'enc_loss': loss_enc.item(), 'dec_loss' : loss_dec.item() }\n",
    "\n",
    "#                     for tag, value in info.items():\n",
    "#                         self.logger.scalar_summary(tag, value, step+1)\n",
    "\n",
    "#                     # 2. Log values and gradients of the parameters (histogram summary)\n",
    "#                     for tag, value in self.net.named_parameters():\n",
    "#                         tag = tag.replace('.', '/')\n",
    "#                         self.logger.histo_summary(tag, value.data.cpu().numpy(), step+1)\n",
    "              \n",
    "            \n",
    "#             self.model_dictionary = {'epoch': epoch + 1,\n",
    "#                     'state_dict_enc': self.encoder.state_dict(),\n",
    "#                     'optimizer_enc': self.optim_enc.state_dict(),\n",
    "#                     'state_dict_dec': self.decoder.state_dict(),\n",
    "#                     'optimizer_dec': self.optim_dec.state_dict(),\n",
    "#                     'args':args,\n",
    "#                     'loss':total_loss\n",
    "#                     }\n",
    "#             self.save_checkpoint(self.args, self.model_dictionary)\n",
    "            total_loss = 0\n",
    "            anomal_score = postprocessing.get_anomalscore_encdec(base_model = self,  \\\n",
    "                                                                 generate_batchfy = self.generate_batchfy, length = 449916,args = self.args)\n",
    "            \n",
    "\n",
    " \n",
    "    def save_checkpoint(self, args, state):\n",
    "        checkpoint = Path(self.base_dir, 'checkpoint')\n",
    "        checkpoint = checkpoint.with_suffix('.pth')\n",
    "        torch.save(state, checkpoint)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postprocessing.evaluate_conv(nomalize_scores = anomal_score, conv = 3, check_step = ,prints= True, endpoint = 449916, args= args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T07:00:42.175859Z",
     "start_time": "2019-03-04T07:00:42.145368Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'anomal_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-141-be51912079ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0manomal_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'anomal_score' is not defined"
     ]
    }
   ],
   "source": [
    "anomal_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, thresholds = metrics.precision_recall_curve(  args['test_y'][:449916].cpu().numpy(), anomal_score,pos_label =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T23:49:39.273136Z",
     "start_time": "2019-03-03T23:49:39.247919Z"
    }
   },
   "outputs": [],
   "source": [
    "args={}\n",
    "args['model_name'] = 'enc_dec'\n",
    "args['cuda'] = True\n",
    "args['batch_size'] = 256\n",
    "args['clip'] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T23:49:40.257852Z",
     "start_time": "2019-03-03T23:49:40.233198Z"
    }
   },
   "outputs": [],
   "source": [
    "import utils.preprocessing as preprocessing\n",
    "import utils.postprocessing as postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T23:49:47.638583Z",
     "start_time": "2019-03-03T23:49:45.568320Z"
    }
   },
   "outputs": [],
   "source": [
    "train_x_batchfy = preprocessing.batchify(args, train_x, args['batch_size'])\n",
    "test_x_batchfy = preprocessing.batchify(args, test_x, args['batch_size'])\n",
    "generate_batchfy = preprocessing.batchify(args, test_x, 1)\n",
    "train_generate_batchfy = preprocessing.batchify(args, train_x, 1)\n",
    "\n",
    "args['test_y'] = test_y\n",
    "args['cell_type'] = 'LSTM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T23:49:50.995099Z",
     "start_time": "2019-03-03T23:49:50.964633Z"
    }
   },
   "outputs": [],
   "source": [
    "train_col = 3\n",
    "args['epoch'] = 1\n",
    "args['seq_length'] = 2\n",
    "args['hidden_size'] = 32\n",
    "args['train_x_batchfy'] = train_x_batchfy[:,:,[0,1,2]]\n",
    "args['test_x_batchfy'] = test_x_batchfy[:,:,[0,1,2]]\n",
    "args['generate_batchfy'] =generate_batchfy[:,:,[0,1,2]]\n",
    "args['train_generate_batchfy'] =train_generate_batchfy[:,:,[0,1,2]]\n",
    "args['data_dim'] = train_col\n",
    "args['dropout'] = 0.5\n",
    "args['nlayers'] = 2\n",
    "args['rnn_inp_size'] = args['data_dim']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T23:52:04.800839Z",
     "start_time": "2019-03-03T23:52:04.645091Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1939\n",
      "1 1939\n",
      "2 1939\n",
      "3 1939\n",
      "4 1939\n",
      "5 1939\n",
      "6 1939\n",
      "7 1939\n",
      "8 1939\n",
      "9 1939\n",
      "10 1939\n",
      "11 1939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mjh319/anaconda3/envs/latest_3.6/lib/python3.6/site-packages/ipykernel_launcher.py:89: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "/home/mjh319/anaconda3/envs/latest_3.6/lib/python3.6/site-packages/ipykernel_launcher.py:104: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
     ]
    }
   ],
   "source": [
    "args['lr'] =0.001\n",
    "solver = Solver(args = args)\n",
    "solver.fit(load = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T12:06:03.810464Z",
     "start_time": "2019-03-03T12:02:48.594505Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anomal_detection...\n"
     ]
    }
   ],
   "source": [
    "import utils.postprocessing as postprocessing\n",
    "anomal_score = postprocessing.get_anomalscore_encdec(base_model = solver,  generate_batchfy = solver.generate_batchfy, length = 449916,args = args)\n",
    "# _,_,_ = utils.evaluate_conv(nomalize_scores = anomal_score ,num_samples = 100000, conv = 0, check_step = 1000, attack_list = attack_list, length = 449916,prints=True,args = args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T12:09:47.988586Z",
     "start_time": "2019-03-03T12:09:47.968236Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(449916, 3)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anomal_score.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T04:14:55.929700Z",
     "start_time": "2019-03-04T04:14:50.284809Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mjh319/workspace/swat_anomal/utils/dataset.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  test_y[test_y == 'Attack'] = 1\n",
      "/home/mjh319/workspace/swat_anomal/utils/dataset.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  test_y[test_y == 'Normal'] = 0\n"
     ]
    }
   ],
   "source": [
    "# parameter\n",
    "train_path= '../../DATA/SWaT/SWaT_Physical/SWaT_Dataset_Normal_v0.csv'\n",
    "test_path = '../../DATA/SWaT/SWaT_Physical/SWaT_Dataset_Attack_v0.csv'\n",
    "train_x, test_x, test_y = dataset.pca_return(train_path = train_path, test_path = test_path)\n",
    "\n",
    "attack_list = '../../DATA/SWaT/SWaT_Physical/attack_list.csv'\n",
    "attack_list = pd.read_csv(attack_list, error_bad_lines=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter\n",
    "train_path= '../../DATA/SWaT/SWaT_Physical/SWaT_Dataset_Normal_v0.csv'\n",
    "test_path = '../../DATA/SWaT/SWaT_Physical/SWaT_Dataset_Attack_v0.csv'\n",
    "attack_list = '../../DATA/SWaT/SWaT_Physical/attack_list.csv'\n",
    "attack_list = pd.read_csv(attack_list, error_bad_lines=False, sep='\\t')\n",
    "\n",
    "args={}\n",
    "args['model_name'] = 'enc_dec'\n",
    "args['cuda'] = True\n",
    "args['batch_size'] = 256\n",
    "args['clip'] = 4\n",
    "args['train_path'] = train_path\n",
    "args['test_path'] = test_path\n",
    "\n",
    "train_x, test_x, test_y = dataset.pca_return(train_path = train_path, test_path = test_path)\n",
    "\n",
    "train_x_batchfy = utils.batchify(args, train_x, args['batch_size'])\n",
    "test_x_batchfy = utils.batchify(args, test_x, args['batch_size'])\n",
    "generate_batchfy = utils.batchify(args, test_x, 1)\n",
    "train_generate_batchfy = utils.batchify(args, train_x, 1)\n",
    "\n",
    "args['test_y'] = test_y\n",
    "args['cell_type'] = 'LSTM'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T06:41:23.311054Z",
     "start_time": "2019-03-04T06:41:23.282654Z"
    }
   },
   "outputs": [],
   "source": [
    "train_path= '../../DATA/SWaT/SWaT_Physical/SWaT_Dataset_Normal_v0.csv'\n",
    "test_path = '../../DATA/SWaT/SWaT_Physical/SWaT_Dataset_Attack_v0.csv'\n",
    "attack_list = '../../DATA/SWaT/SWaT_Physical/attack_list.csv'\n",
    "attack_list = pd.read_csv(attack_list, error_bad_lines=False, sep='\\t')\n",
    "\n",
    "args={}\n",
    "args['model_name'] = 'enc_dec'\n",
    "args['cuda'] = True\n",
    "args['batch_size'] = 256\n",
    "args['clip'] = 4\n",
    "args['train_path'] = train_path\n",
    "args['test_path'] = test_path\n",
    "args['dropout'] = 0.5\n",
    "args['hidden_size'] = 2\n",
    "args['data_dim'] = 2\n",
    "args['rnn_inp_size'] = args['data_dim']\n",
    "args['nlayers'] = 2\n",
    "args['lr'] =0.001\n",
    "args['cell_type'] = 'LSTM'\n",
    "args['epoch'] = 1\n",
    "args['seq_length'] = 2\n",
    "args['selected_dim'] = [0, 39]\n",
    "args['evaluate'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T06:34:02.350438Z",
     "start_time": "2019-03-06T06:33:55.884500Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mjh319/workspace/swat_anomal/utils/dataset.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  test_y[test_y == 'Attack'] = 1\n",
      "/home/mjh319/workspace/swat_anomal/utils/dataset.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  test_y[test_y == 'Normal'] = 0\n"
     ]
    }
   ],
   "source": [
    "solver = Solver(args = args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T06:36:56.247303Z",
     "start_time": "2019-03-06T06:34:02.443270Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post in\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'anomal_score' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-172-af2129f9fdbd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-170-8c6c34b2bce4>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, load)\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"post in\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manomal_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpostprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_anomalscore_minmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m                                                                   \u001b[0mgenerate_batchfy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_batchfy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m449916\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manomal_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manomal_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manomal_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manomal_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'anomal_score' referenced before assignment"
     ]
    }
   ],
   "source": [
    "solver.fit(load = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T06:37:17.639426Z",
     "start_time": "2019-03-06T06:37:17.608631Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[41.15925   , -0.89002657],\n",
       "       [41.0561    , -0.9947951 ],\n",
       "       [41.299503  , -0.87507385],\n",
       "       ...,\n",
       "       [60.857098  , -0.9810451 ],\n",
       "       [61.40289   , -0.87138444],\n",
       "       [61.910927  , -0.98094594]], dtype=float32)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solver.anomal_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T06:39:21.314499Z",
     "start_time": "2019-03-06T06:39:21.289030Z"
    }
   },
   "outputs": [],
   "source": [
    "anomal_score = LA.norm(solver.anomal_score, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T06:45:07.848467Z",
     "start_time": "2019-03-06T06:45:07.733313Z"
    }
   },
   "outputs": [],
   "source": [
    "precision, recall, thresholds = metrics.precision_recall_curve(  solver.test_y[:449916].values.astype(int), anomal_score,pos_label =1)\n",
    "beta = 1\n",
    "f1 = (1+beta**2)*(precision*recall)/((beta**2*precision)+recall)\n",
    "f1 = np.nan_to_num(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T06:45:17.140669Z",
     "start_time": "2019-03-06T06:45:17.118531Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.2165193 , 0.21651577, 0.21651223, ..., 0.        , 0.        ,\n",
       "       0.        ])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T06:45:57.667488Z",
     "start_time": "2019-03-06T06:45:57.642235Z"
    }
   },
   "outputs": [],
   "source": [
    "max_f1 = f1[np.argmax(f1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T06:46:10.877179Z",
     "start_time": "2019-03-06T06:46:10.850689Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6225012156845409"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T06:43:54.843851Z",
     "start_time": "2019-03-06T06:43:54.811867Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(444639,)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, thresholds = metrics.precision_recall_curve(  self.test_y[:449916].values.astype(int), self.anomal_score,pos_label =1)\n",
    "beta = 1\n",
    "f1 = (1+beta**2)*(precision*recall)/((beta**2*precision)+recall)\n",
    "f1 = np.nan_to_num(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T06:41:51.176043Z",
     "start_time": "2019-03-06T06:41:51.152993Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(solver.test_y[:449916].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T06:28:57.173737Z",
     "start_time": "2019-03-06T06:28:57.151293Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=object)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solver.test_y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T07:15:49.694135Z",
     "start_time": "2019-03-04T07:15:49.484630Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=object)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(solver.test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args['hidden_size'] = 2\n",
    "candi_front = [0, 1, 2]\n",
    "candi_end = [38,39,40,41]\n",
    "for i in range(len(candi_front)):\n",
    "    for j in range(len(candi_end)):\n",
    "        first = candi_front[i]\n",
    "        second = candi_end[j]\n",
    "        print(\"first dim = {}, end dim = {}\".format(first, second ))\n",
    "        train_col = 2\n",
    "        args['epoch'] = 25\n",
    "        args['seq_length'] = 2\n",
    "        args['train_x_batchfy'] = train_x_batchfy[:,:,[first,second]]\n",
    "        args['test_x_batchfy'] = test_x_batchfy[:,:,[first,second]]\n",
    "        args['generate_batchfy'] =generate_batchfy[:,:,[first,second]]\n",
    "        args['train_generate_batchfy'] =train_generate_batchfy[:,:,[first,second]]\n",
    "        args['data_dim'] = train_col\n",
    "        args['dropout'] = 0.5\n",
    "        args['nlayers'] = 2\n",
    "        args['rnn_inp_size'] = args['data_dim']\n",
    "        args['lr'] =0.001\n",
    "        solver = Solver(args = args)\n",
    "        \n",
    "#         try:\n",
    "#             checkpoint = torch.load(Path(str(k) +','+str(k+1) +', enc, hid='+str(2)).with_suffix('.pth'))\n",
    "#             test.encoder.load_state_dict(checkpoint['state_dict_enc'])\n",
    "#             test.optim_enc.load_state_dict((checkpoint['optimizer_enc']))\n",
    "#             test.decoder.load_state_dict(checkpoint['state_dict_dec'])\n",
    "#             test.optim_dec.load_state_dict((checkpoint['optimizer_dec']))    \n",
    "#         except:\n",
    "#             print(\"not load\")\n",
    "\n",
    "        solver.fit(load = False)\n",
    "\n",
    "        anomal_score_test = utils.get_anomalscore_encdec(base_model = test,  generate_batchfy = test.generate_batchfy, length = 449916,args = args)\n",
    "        for conv in range(3):\n",
    "            evaluate_conv2(nomalize_scores = anomal_score_test, conv = conv_candi[conv], prints= True, endpoint = 449916, args= args)\n",
    "\n",
    "        model_dictionary = {'state_dict_enc': test.encoder.state_dict(),\n",
    "        'optimizer_enc': test.optim_enc.state_dict(),\n",
    "        'state_dict_dec': test.decoder.state_dict(),\n",
    "        'optimizer_dec': test.optim_dec.state_dict(),\n",
    "        'args':args,\n",
    "        'anomal_score':anomal_score_test\n",
    "        }\n",
    "        checkpoint = Path(str(k) +','+str(k+1)+', enc, hid='+str(args['hidden_size']))\n",
    "        checkpoint = checkpoint.with_suffix('.pth')\n",
    "        torch.save(model_dictionary, checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T06:32:54.160193Z",
     "start_time": "2019-03-04T06:32:54.133776Z"
    }
   },
   "outputs": [],
   "source": [
    "outSeq = []\n",
    "scores = []\n",
    "solver.encoder.eval()\n",
    "solver.decoder.eval()\n",
    "hidden_enc = solver.encoder.init_hidden(1)\n",
    "endPoint = 2000 #446000 \n",
    "step = args['seq_length']\n",
    "feature_dim = generate_batchfy.size(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T06:32:55.084436Z",
     "start_time": "2019-03-04T06:32:54.286035Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(0,endPoint,step):\n",
    "    Outputseq_enc, hidden_enc = solver.encoder.forward(Variable(args['generate_batchfy'] [i:i+step]).cuda(), hidden_enc)\n",
    "    deccoder_input = Variable(torch.zeros(Outputseq_enc.size())).cuda()\n",
    "    deccoder_input[0,:,:] = Outputseq_enc[-1,:,:]\n",
    "    try:\n",
    "        deccoder_input[1:,:,:] = args['generate_batchfy'] [i+step:i+step+step-1,:,:]\n",
    "    except:\n",
    "        continue\n",
    "    Outputseq_enc, hidden_enc = solver.decoder.forward(deccoder_input, hidden_enc, return_hiddens=True)\n",
    "\n",
    "    if Outputseq_enc.size()[0] != args['generate_batchfy'] [i:i+step].size()[0] :\n",
    "        continue\n",
    "    error = torch.abs(torch.add( Outputseq_enc.view(-1,args['generate_batchfy'] .size(-1)).cpu(), \\\n",
    "                            -args['generate_batchfy'] [i:i+step].view(-1,args['generate_batchfy'] .size(-1)).cpu()))       \n",
    "    outSeq.append(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T06:32:56.040053Z",
     "start_time": "2019-03-04T06:32:56.015739Z"
    }
   },
   "outputs": [],
   "source": [
    "outSeq = torch.cat(outSeq,dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T06:33:43.672898Z",
     "start_time": "2019-03-04T06:33:43.646223Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0169, 1.3731],\n",
       "        [1.0903, 1.3254],\n",
       "        [1.0688, 1.3496],\n",
       "        ...,\n",
       "        [0.8019, 0.9033],\n",
       "        [0.7627, 0.9396],\n",
       "        [0.7892, 0.9052]], grad_fn=<CatBackward>)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outSeq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T06:49:31.753813Z",
     "start_time": "2019-03-04T06:49:31.729315Z"
    }
   },
   "outputs": [],
   "source": [
    "outSeq2 = outSeq.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T06:49:36.736647Z",
     "start_time": "2019-03-04T06:49:36.713033Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 2)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outSeq2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T06:50:18.278542Z",
     "start_time": "2019-03-04T06:50:18.253096Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00018640503"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outSeq2.min(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T06:50:32.186413Z",
     "start_time": "2019-03-04T06:50:32.158850Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0167469 , 1.2815422 ],\n",
       "       [1.0901258 , 1.2338567 ],\n",
       "       [1.0687114 , 1.2580656 ],\n",
       "       ...,\n",
       "       [0.8017858 , 0.8117337 ],\n",
       "       [0.7625318 , 0.84806687],\n",
       "       [0.78907084, 0.8136948 ]], dtype=float32)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outSeq2 - outSeq2.min(axis=0) / (outSeq2.max(axis=0) - outSeq2.min(axis=0) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T06:38:16.933989Z",
     "start_time": "2019-03-04T06:38:16.906764Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0167, 1.2815],\n",
       "        [1.0901, 1.2339],\n",
       "        [1.0687, 1.2581],\n",
       "        ...,\n",
       "        [0.8018, 0.8117],\n",
       "        [0.7625, 0.8481],\n",
       "        [0.7891, 0.8137]], grad_fn=<ThSubBackward>)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outSeq - outSeq.min(dim=0)[0] / (outSeq.max(dim=0)[0]  - outSeq.min(dim=0)[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T06:34:43.289682Z",
     "start_time": "2019-03-04T06:34:43.264393Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.0002, 0.5333], grad_fn=<MinBackward0>), tensor([897, 709]))"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outSeq.min(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,endPoint,step):\n",
    "    Outputseq_enc, hidden_enc = base_model.encoder.forward(Variable(generate_batchfy[i:i+step]).cuda(), hidden_enc)\n",
    "    deccoder_input = Variable(torch.zeros(Outputseq_enc.size())).cuda()\n",
    "    deccoder_input[0,:,:] = Outputseq_enc[-1,:,:]\n",
    "    try:\n",
    "        deccoder_input[1:,:,:] = generate_batchfy[i+step:i+step+step-1,:,:]\n",
    "    except:\n",
    "        continue\n",
    "    Outputseq_enc, hidden_enc = base_model.decoder.forward(deccoder_input, hidden_enc, return_hiddens=True)\n",
    "    if Outputseq_enc.size()[0] != generate_batchfy[i:i+step].size()[0] :\n",
    "        continue\n",
    "    error = torch.abs(torch.add( Outputseq_enc.view(-1,generate_batchfy.size(-1)).cpu(), \\\n",
    "                            -generate_batchfy[i:i+step].view(-1,generate_batchfy.size(-1)).cpu()))       \n",
    "    outSeq.append(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T12:10:02.211696Z",
     "start_time": "2019-03-03T12:10:02.192808Z"
    }
   },
   "outputs": [],
   "source": [
    "import logging as log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T12:11:21.750379Z",
     "start_time": "2019-03-03T12:11:21.730365Z"
    }
   },
   "outputs": [],
   "source": [
    "from logging import handlers\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T12:10:20.975151Z",
     "start_time": "2019-03-03T12:10:20.953489Z"
    }
   },
   "outputs": [],
   "source": [
    "log.basicConfig(filename='./log.txt', level=log.DEBUG)\n",
    "\n",
    "log.debug('debug')\n",
    "log.info('info')\n",
    "log.warning('warning')\n",
    "log.error('error')\n",
    "log.critical('critical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T12:16:39.020728Z",
     "start_time": "2019-03-03T12:16:38.997047Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Logger' object has no attribute 'Formatter'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-122-200c2bd0bb5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcarLogFormatter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFormatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%(asctime)s,%(message)s'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Logger' object has no attribute 'Formatter'"
     ]
    }
   ],
   "source": [
    "carLogFormatter = log.Formatter('%(asctime)s,%(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T12:13:38.064950Z",
     "start_time": "2019-03-03T12:13:38.044125Z"
    }
   },
   "outputs": [],
   "source": [
    "#handler settings\n",
    "carLogHandler = handlers.TimedRotatingFileHandler(filename='car.log', interval=1, encoding='utf-8')\n",
    "carLogHandler.setFormatter(carLogFormatter)\n",
    "# carLogHandler.suffix = \"%Y%m%d\"\n",
    "\n",
    "#logger set\n",
    "carLogger = logging.getLogger()\n",
    "carLogger.setLevel(logging.INFO)\n",
    "carLogger.addHandler(carLogHandler)\n",
    "\n",
    "#use logger\n",
    "carLogger.info(\"car is coming22\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T12:16:46.363967Z",
     "start_time": "2019-03-03T12:16:46.342163Z"
    }
   },
   "outputs": [],
   "source": [
    "carLogFormatter = logging.Formatter('%(asctime)s,%(message)s')\n",
    "log = logging.getLogger('snowdeer_log')\n",
    "log.setLevel(logging.DEBUG)\n",
    "\n",
    "fileHandler = logging.FileHandler('./log.txt')\n",
    "fileHandler.setFormatter(carLogFormatter)\n",
    "streamHandler = logging.StreamHandler()\n",
    "\n",
    "\n",
    "log.addHandler(fileHandler)\n",
    "log.addHandler(streamHandler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T12:16:49.596393Z",
     "start_time": "2019-03-03T12:16:49.573324Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "info\n",
      "info\n"
     ]
    }
   ],
   "source": [
    "log.info('info')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
