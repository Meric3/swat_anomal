{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T06:44:56.923952Z",
     "start_time": "2019-04-16T06:44:56.892529Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import utils.dataset as dataset\n",
    "import utils.preprocessing as preprocessing\n",
    "from utils.logger import Logger\n",
    "import lstm.model as model\n",
    "import utils.postprocessing as postprocessing\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from numpy import linalg as LA\n",
    "import numpy as np\n",
    "import logging\n",
    "import argparse\n",
    "\n",
    "\n",
    "from astropy.convolution import Gaussian1DKernel, convolve\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T05:42:56.659760Z",
     "start_time": "2019-04-16T05:42:56.634712Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import sklearn.preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(5, shuffle=False, random_state=0)\n",
    "\n",
    "cv_list = []\n",
    "\n",
    "for i, (idx_train, idx_test) in enumerate(cv.split(attack_data)):\n",
    "    cv_list.append([idx_train, idx_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T06:02:06.675695Z",
     "start_time": "2019-02-28T06:02:06.658312Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-23T03:05:54.436270Z",
     "start_time": "2019-03-23T03:05:54.403633Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-23T03:05:59.837268Z",
     "start_time": "2019-03-23T03:05:59.812761Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1553310354.434949\n"
     ]
    }
   ],
   "source": [
    "print(start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-23T03:06:26.112772Z",
     "start_time": "2019-03-23T03:06:26.055913Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1553310368.0227296\n"
     ]
    }
   ],
   "source": [
    "print(end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-23T03:06:08.024051Z",
     "start_time": "2019-03-23T03:06:07.999885Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-23T03:06:36.103032Z",
     "start_time": "2019-03-23T03:06:36.081942Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.587780714035034\n"
     ]
    }
   ],
   "source": [
    "print(end_time -start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T06:35:42.764950Z",
     "start_time": "2019-02-28T06:35:42.717765Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]predict 빈도: {-1: 4842, 1: 5158}\n"
     ]
    }
   ],
   "source": [
    "# comp_num = 45\n",
    "clf = svm.OneClassSVM(nu=0.001, kernel=\"rbf\", gamma=0.0001, verbose=True)\n",
    "clf.fit(train_x[0:20000,0:2])\n",
    "preds = clf.predict(test_x[5000:15000, 0:2]) \n",
    "unique, counts = np.unique(preds, return_counts=True)\n",
    "print('predict 빈도:', dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T06:10:27.161686Z",
     "start_time": "2019-02-28T06:10:27.112893Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]predict 빈도: {-1: 2390, 1: 7610}\n"
     ]
    }
   ],
   "source": [
    "# comp_num = 45\n",
    "clf = svm.OneClassSVM(nu=0.001, kernel=\"rbf\", gamma=0.0001, verbose=True)\n",
    "clf.fit(train_x[0:20000,0:4])\n",
    "preds = clf.predict(test_x[5000:15000, 0:4]) \n",
    "unique, counts = np.unique(preds, return_counts=True)\n",
    "print('predict 빈도:', dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T06:10:00.668563Z",
     "start_time": "2019-02-28T06:10:00.628819Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]predict 빈도: {-1: 4842, 1: 5158}\n"
     ]
    }
   ],
   "source": [
    "# comp_num = 2\n",
    "clf = svm.OneClassSVM(nu=0.001, kernel=\"rbf\", gamma=0.0001, verbose=True)\n",
    "clf.fit(train_x[0:20000,:])\n",
    "preds = clf.predict(test_x[5000:15000, :]) \n",
    "unique, counts = np.unique(preds, return_counts=True)\n",
    "print('predict 빈도:', dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-10T05:04:33.683604Z",
     "start_time": "2019-03-10T05:04:33.654325Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class ENCODER(nn.Module):\n",
    "\n",
    "    def __init__(self,args):\n",
    "        super(ENCODER, self).__init__()\n",
    "        self.args = args\n",
    "        self.drop = nn.Dropout(args['dropout'])\n",
    "        self.linear = nn.Linear(args['hidden_size'], args['data_dim'])\n",
    "\n",
    "        if args['cell_type'] in ['LSTM', 'GRU']:\n",
    "            self.rnn = getattr(nn, args['cell_type'])(args['rnn_inp_size'], args['hidden_size'], args['nlayers'], dropout=args['dropout'])\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
    "        self.linear.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, input, hidden, return_hiddens=False, noise=False):\n",
    "        output, hidden = self.rnn(input, hidden)\n",
    "        output = self.linear(output.contiguous().view(-1,self.args['hidden_size']))\n",
    "        output = output.contiguous().view(input.size()[0], -1, self.args['rnn_inp_size'])\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters()).data ############# 이게 무엇\n",
    "        if self.args['cell_type'] == 'LSTM':\n",
    "            return (Variable(weight.new(self.args['nlayers'], bsz, self.args['hidden_size']).zero_()),\n",
    "                    Variable(weight.new(self.args['nlayers'], bsz, self.args['hidden_size']).zero_()))\n",
    "\n",
    "    def repackage_hidden(self,h):\n",
    "        \"\"\"Wraps hidden states in new Variables, to detach them from their history.\"\"\"\n",
    "        if type(h) == tuple:\n",
    "            return tuple(self.repackage_hidden(v) for v in h)\n",
    "        else:\n",
    "            return Variable(h.data)\n",
    "\n",
    "    def extract_hidden(self, hidden):\n",
    "        if self.args['cell_type'] == 'LSTM':\n",
    "            return hidden[0][-1].data.cpu()  # hidden state last layer (hidden[1] is cell state)\n",
    "        else:\n",
    "            return hidden[-1].data.cpu()  # last layer\n",
    "\n",
    "        \n",
    "class DECODER(nn.Module):\n",
    "\n",
    "    def __init__(self,args):\n",
    "        super(DECODER, self).__init__()\n",
    "        self.args = args\n",
    "        self.drop = nn.Dropout(args['dropout'])\n",
    "        self.linear = nn.Linear(args['hidden_size'], args['data_dim'])\n",
    "\n",
    "        if args['cell_type'] in ['LSTM', 'GRU']:\n",
    "            self.rnn = getattr(nn, args['cell_type'])(args['rnn_inp_size'], args['hidden_size'], args['nlayers'], dropout=args['dropout'])\n",
    "\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
    "        self.linear.bias.data.fill_(0)\n",
    "        \n",
    "    def forward(self, input, hidden, return_hiddens=False, noise=False):\n",
    "        \n",
    "        output, hidden = self.rnn(input, hidden)\n",
    "        output = self.linear(output.contiguous().view(-1,self.args['hidden_size']))\n",
    "        output = output.contiguous().view(input.size()[0], -1, self.args['rnn_inp_size'])\n",
    "\n",
    "        return output, hidden\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters()).data ############# 이게 무엇\n",
    "        if self.args['cell_type'] == 'LSTM':\n",
    "            return (Variable(weight.new(self.args['nlayers'], bsz, self.args['hidden_size']).zero_()),\n",
    "                    Variable(weight.new(self.args['nlayers'], bsz, self.args['hidden_size']).zero_()))\n",
    "\n",
    "    def repackage_hidden(self,h):\n",
    "        \"\"\"Wraps hidden states in new Variables, to detach them from their history.\"\"\"\n",
    "        if type(h) == tuple:\n",
    "            return tuple(self.repackage_hidden(v) for v in h)\n",
    "        else:\n",
    "            return Variable(h.data)\n",
    "\n",
    "    def extract_hidden(self, hidden):\n",
    "        if self.args['cell_type'] == 'LSTM':\n",
    "            return hidden[0][-1].data.cpu()  # hidden state last layer (hidden[1] is cell state)\n",
    "        else:\n",
    "            return hidden[-1].data.cpu()  # last layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-17T04:39:13.460020Z",
     "start_time": "2019-04-17T04:39:13.422190Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Solver():\n",
    "    def __init__(self, args):\n",
    "        \n",
    "        torch.manual_seed(777)\n",
    "        torch.cuda.manual_seed_all(777)\n",
    "        np.random.seed(777)\n",
    "        \n",
    "        self.attack_list = pd.read_csv(args.attack_list_path, error_bad_lines=False, sep='\\t')\n",
    "\n",
    "        self.tf_log = args.tf_log\n",
    "        \n",
    "        cv = KFold(5, shuffle=False, random_state=0)\n",
    "\n",
    "\n",
    "        \n",
    "        train_x, test_x, test_y = dataset.lstm_dataset(train_path = args.train_path, test_path = args.test_path)\n",
    "        \n",
    "        cv_list = []\n",
    "        cv = KFold(5, shuffle=False, random_state=0)\n",
    "        for i, (idx_train, idx_test) in enumerate(cv.split(test_x)):\n",
    "            cv_list.append([idx_train, idx_test])\n",
    "            \n",
    "        self.cv_list = cv_list      \n",
    "        \n",
    "        train_x_batchfy = preprocessing.batchify(args, train_x, args.batch_size)\n",
    "        test_x_batchfy = preprocessing.batchify(args, test_x, args.batch_size)\n",
    "        generate_batchfy = preprocessing.batchify(args, test_x, 1)\n",
    "        train_generate_batchfy = preprocessing.batchify(args, train_x, 1)\n",
    "        \n",
    "        self.train_x_batchfy = train_x_batchfy[:,:,args.selected_dim]\n",
    "        self.test_x_batchfy = test_x_batchfy[:,:,args.selected_dim]\n",
    "        self.generate_batchfy = generate_batchfy[:,:,args.selected_dim]\n",
    "        self.train_generate_batchfy = train_generate_batchfy[:,:,args.selected_dim]\n",
    "        self.test_y = test_y\n",
    "        \n",
    "\n",
    "        self.args = args\n",
    "        self.encoder = model.ENCODER(self.args)\n",
    "        self.encoder.cuda()\n",
    "\n",
    "        self.decoder = model.DECODER(self.args)\n",
    "        self.decoder.cuda()\n",
    "\n",
    "        self.optim_enc   = torch.optim.Adam(self.encoder.parameters(), self.args.lr)\n",
    "        self.optim_dec   = torch.optim.Adam(self.decoder.parameters(), self.args.lr)\n",
    "\n",
    "        self.loss_fn = nn.MSELoss()    \n",
    "    \n",
    "        self.logger = Logger('./tf_logs')\n",
    "    \n",
    "        self.base_dir = Path('model_save')\n",
    "        self.base_dir.mkdir(parents=True,exist_ok=True)      \n",
    "        \n",
    "#         self.evaluate = args.evaluate\n",
    "    \n",
    "#     def make_dir_name(self, args):\n",
    "#         return 'modelName:'+args.model_name+'__cellType:'+args.cell_type \\\n",
    "#                 + '__hidSize:' + str(args.hidden_size) + '__dropout:' + str(args.dropout)\n",
    "\n",
    "\n",
    "\n",
    "#TODO 로그 정리\n",
    "#         self.log = logging.getLogger('LSTM_log')\n",
    "#         self.log.setLevel(logging.DEBUG)\n",
    "\n",
    "#         formatter = logging.Formatter('%(asctime)s > %(message)s')\n",
    "\n",
    "#         fileHandler = logging.FileHandler('./log/logger.txt')\n",
    "\n",
    "#         fileHandler.setFormatter(formatter)\n",
    "#         self.log.addHandler(fileHandler)\n",
    "        \n",
    "#         self.log.critical(\"Selected dim : \" + str(args.selected_dim))\n",
    "        log.critical(\"\\n\")\n",
    "        log.critical(\"Selected dim : \" + str(args.selected_dim))\n",
    "\n",
    "\n",
    "\n",
    "    def load(self, path):\n",
    "        try:\n",
    "#             checkpoint = torch.load(Path(self.base_dir, str(args.selected_dim)))\n",
    "#             checkpoint = checkpoint.with_suffix('.pth')\n",
    "#             start_epoch = checkpoint['epoch\n",
    "#             self.encoder.load_state_dict(checkpoint['state_dict_enc)\n",
    "#             self.optim_enc.load_state_dict((checkpoint['optimizer_enc))\n",
    "#             self.decoder.load_state_dict(checkpoint['state_dict_dec)\n",
    "#             self.optim_dec.load_state_dict((checkpoint['optimizer_dec))            \n",
    "#             del checkpoint\n",
    "            print(\"=> loaded checkpoint\")\n",
    "        except:\n",
    "            print(\"=> Not exist checkpoint\")\n",
    "            pass        \n",
    "\n",
    "    def fit(self, load):\n",
    "        total_loss = 0\n",
    "        max_f1 = 0\n",
    "        total_length = self.train_x_batchfy.size(0) - 1\n",
    "        start_time = time.time()\n",
    "        \n",
    "        \n",
    "                                          \n",
    "        for epoch in range(0, self.args.epoch):\n",
    "\n",
    "            self.encoder.train()\n",
    "            self.decoder.train()\n",
    "                \n",
    "            hidden_enc = self.encoder.init_hidden(self.args.batch_size)\n",
    "\n",
    "#             for batch, i in enumerate(range(0, self.train_x_batchfy.size(0) - 1, self.args.seq_length)):\n",
    "#                 outSeq = []\n",
    "#                 inputSeq, targetSeq = preprocessing.get_batch(self.args, self.train_x_batchfy, i)\n",
    "\n",
    "#                 if args.seq_length != targetSeq.size()[0] :\n",
    "#                     continue\n",
    "#                 hidden_enc = self.encoder.repackage_hidden(hidden_enc)\n",
    "#                 self.optim_enc.zero_grad()\n",
    "#                 self.optim_dec.zero_grad()\n",
    "                \n",
    "#                 Outputseq_enc, hidden_enc = self.encoder.forward(inputSeq, hidden_enc, return_hiddens=True)\n",
    "#                 deccoder_input = Variable(torch.zeros(Outputseq_enc.size())).cuda()\n",
    "                \n",
    "#                 deccoder_input[0,:,:] = Outputseq_enc[-1,:,:] # inputSeq[-1,:,:]\n",
    "#                 deccoder_input[1:,:,:] = targetSeq[:-1,:,:]\n",
    "                \n",
    "#                 loss_enc = self.loss_fn(Outputseq_enc[-1,:,:].view(self.args.batch_size, -1), targetSeq[0,:,:].contiguous().view(self.args.batch_size, -1))\n",
    "#                 loss_enc.backward(retain_graph=True)\n",
    "                \n",
    "                \n",
    "#                 encoder_norm = sum(p.grad.data.abs().sum() for p in self.encoder.parameters())\n",
    "                \n",
    "#                 torch.nn.utils.clip_grad_norm_(self.encoder.parameters(), self.args.clip)\n",
    "                \n",
    "#                 self.optim_enc.step()     \n",
    "                \n",
    "#                 Outputseq_enc, hidden_enc = self.decoder.forward(deccoder_input, hidden_enc, return_hiddens=True)\n",
    "#                 loss_dec = self.loss_fn(Outputseq_enc.view(args.batch_size, -1), targetSeq.contiguous().view(args.batch_size, -1))   \n",
    "#                 loss_dec.backward()\n",
    "                \n",
    "#                 edecoder_norm = sum(p.grad.data.abs().sum() for p in self.decoder.parameters())\n",
    "                \n",
    "                \n",
    "#                 torch.nn.utils.clip_grad_norm_(self.decoder.parameters(), self.args.clip)\n",
    "#                 self.optim_dec.step()\n",
    "                \n",
    "                \n",
    "                \n",
    "#                 total_loss += loss_enc.item() + loss_dec.item()        \n",
    "\n",
    "#                 if batch % 30 == 0 and self.tf_log == True :\n",
    "#                     print(encoder_norm)\n",
    "#                     print(decoder_norm)\n",
    "#                     # 1. Log scalar values (scalar summary)\n",
    "#                     info = { 'enc_loss': loss_enc.item(), 'dec_loss' : loss_dec.item() }\n",
    "\n",
    "#                     for tag, value in info.items():\n",
    "#                         self.logger.scalar_summary(tag, value, epoch*total_length + i +1)\n",
    "\n",
    "#                     # 2. Log values and gradients of the parameters (histogram summary)\n",
    "#                     for tag, value in self.encoder.named_parameters():\n",
    "#                         tag = tag.replace('.', '/')\n",
    "#                         self.logger.histo_summary(tag, value.data.cpu().numpy(), epoch*total_length + i +1)\n",
    "                        \n",
    "#                     for tag, value in self.decoder.named_parameters():\n",
    "#                         tag = tag.replace('.', '/')\n",
    "#                         self.logger.histo_summary(tag, value.data.cpu().numpy(), epoch*total_length + i +1)\n",
    "            \n",
    "            total_loss = 0    \n",
    "            if len(self.args.selected_dim) == 1:\n",
    "                self.anomal_score = postprocessing.get_anomalscore_encdec_1dim(base_model = self,  \\\n",
    "                                                                 generate_batchfy = self.generate_batchfy, length = 449916,args = self.args, cv_list = self.cv_list)\n",
    "            else:    \n",
    "                self.anomal_score = postprocessing.get_anomalscore_encdec(base_model = self,  \\\n",
    "                                                                 generate_batchfy = self.generate_batchfy, length = 449916,args = self.args, cv_list = self.cv_list)\n",
    "\n",
    "            self.anomal_score = LA.norm(self.anomal_score, axis=1)\n",
    "    #TODO conv 설정\n",
    "#             max_conv, max_pre, max_recall, max_f1_tp, max_zerolist, find_attack_list = postprocessing.evaluate_conv(self.anomal_score, self.test_y, self.attack_list, 400)\n",
    "    \n",
    "          \n",
    "#             if max_f1_tp > max_f1:\n",
    "#                 end_time = time.time()\n",
    "                \n",
    "# #                 print(\"epoch[{}]\\t conv[{}]\\t precision[{}]\\t recall[{}]\\t f1[{}]\\t findnum[{}]\\t time[{}]\".format(epoch, max_conv, max_pre, max_recall, max_f1_tp,36 - max_zerolist, end_time - start_time))\n",
    "                \n",
    "#                 #TODP self로 바꿔야함\n",
    "#                 log.info(\"epoch[{}]\\t conv[{}]\\t precision[{}]\\t recall[{}]\\t f1[{}]\\t findnum[{}]\\t time[{}]\".format(epoch, max_conv, max_pre, max_recall, max_f1_tp,36 - max_zerolist, end_time - start_time))\n",
    "#                 log.info(find_attack_list)\n",
    "                \n",
    "#                 self.model_dictionary = {'epoch': epoch,\n",
    "#                         'state_dict_enc': self.encoder.state_dict(),\n",
    "#                         'optimizer_enc': self.optim_enc.state_dict(),\n",
    "#                         'state_dict_dec': self.decoder.state_dict(),\n",
    "#                         'optimizer_dec': self.optim_dec.state_dict(),\n",
    "#                         'args':args,\n",
    "#                         'loss':total_loss,\n",
    "#                         'anomal_score' : self.anomal_score ,\n",
    "#                          'conv' : max_conv,\n",
    "#                         'f1' : max_f1_tp,\n",
    "#                          'find_attack_list': find_attack_list                \n",
    "#                         }\n",
    "#                 self.save_checkpoint(self.args, self.model_dictionary)\n",
    "#                 max_f1 = max_f1_tp\n",
    "                                    \n",
    "            \n",
    "            \n",
    "            \n",
    " \n",
    "    def save_checkpoint(self, args, state):\n",
    "        checkpoint = Path(self.base_dir, str(args.selected_dim))\n",
    "        checkpoint = checkpoint.with_suffix('.pth')\n",
    "        torch.save(state, checkpoint)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-17T04:39:14.113320Z",
     "start_time": "2019-04-17T04:39:14.086345Z"
    }
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\"--cuda\", default=True, action=\"store_true\")\n",
    "parser.add_argument(\"--tf_log\", default=False, action=\"store_true\")\n",
    "parser.add_argument(\"--model_name\", type=str, default=\"enc_dec\")\n",
    "parser.add_argument(\"--batch_size\", type=int, default=256)\n",
    "parser.add_argument(\"--clip\", type=int, default=1)\n",
    "\n",
    "parser.add_argument(\"--train_path\", type=str, default=\"../../DATA/SWaT/SWaT_Physical/SWaT_Dataset_Normal_v0.csv\")\n",
    "parser.add_argument(\"--test_path\", type=str, default=\"../../DATA/SWaT/SWaT_Physical/SWaT_Dataset_Attack_v0.csv\")\n",
    "parser.add_argument(\"--attack_list_path\", type=str, default='../../DATA/SWaT/SWaT_Physical/attack_list.csv')\n",
    "\n",
    "parser.add_argument(\"--dropout\", type=float, default=0.5)\n",
    "parser.add_argument(\"--hidden_size\", type=int, default=128)\n",
    "parser.add_argument(\"--nlayers\", type=int, default=2)\n",
    "parser.add_argument(\"--lr\", type=float, default=0.0001)\n",
    "\n",
    "parser.add_argument(\"--cell_type\", type=str, default=\"LSTM\")\n",
    "parser.add_argument(\"--epoch\", type=int, default=1)\n",
    "parser.add_argument(\"--seq_length\", type=int, default=2)\n",
    "parser.add_argument('--selected_dim', nargs='+', type=int, default=[36, 38, 28, 40])\n",
    "args = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-17T04:39:20.094252Z",
     "start_time": "2019-04-17T04:39:14.340348Z"
    }
   },
   "outputs": [],
   "source": [
    "args.selected_dim = [0, 40]\n",
    "args.hidden_size = 128\n",
    "solver = Solver(args = args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-17T04:39:14.614Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([89984, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "import utils.postprocessing as postprocessing\n",
    "solver.fit(load = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T06:03:49.727208Z",
     "start_time": "2019-04-16T06:03:49.698881Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solver.test_y[solver.cv_list[0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T06:45:57.843277Z",
     "start_time": "2019-04-16T06:45:36.741Z"
    }
   },
   "outputs": [],
   "source": [
    "max_conv, max_pre, max_recall, max_f1_tp, max_thresholds\\\n",
    ",precision, recall, f1, thresholds= evaluate_conv(solver.anomal_score, solver.test_y[solver.cv_list[0][1]], solver.attack_list, 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T06:32:42.770779Z",
     "start_time": "2019-04-16T06:32:42.735727Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def evaluate_conv(anomal_score, test_y, attack_list, conv):\n",
    "    max_f1 = 0\n",
    "    for conv in range(0, conv, 10):\n",
    "        if conv != 0:\n",
    "            gauss_kernel = Gaussian1DKernel(conv)\n",
    "            norm = convolve(anomal_score, gauss_kernel)\n",
    "        else:\n",
    "            norm = anomal_score\n",
    "\n",
    "        precision, recall, thresholds = metrics.precision_recall_curve( test_y.cpu().numpy(), norm ,pos_label =1)\n",
    "\n",
    "        beta = 1\n",
    "        f1 = (1+beta**2)*(precision*recall)/((beta**2*precision)+recall)\n",
    "        f1 = np.nan_to_num(f1)\n",
    "\n",
    "        \n",
    "        \n",
    "        max_pre_tp = precision[np.argmax(f1)]\n",
    "        max_recall_tp = recall[np.argmax(f1)]\n",
    "        max_f1_tp = f1[np.argmax(f1)]\n",
    "        thresholds_tp = thresholds[np.argmax(f1)]\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        if max_f1 < max_f1_tp:\n",
    "            max_pre = max_pre_tp\n",
    "            max_recall = max_recall_tp\n",
    "            max_f1 = max_f1_tp\n",
    "            max_conv = conv\n",
    "            max_thresholds = thresholds_tp\n",
    "\n",
    "#     print(\"conv[{}]\\t precision[{}]\\t recall[{}]\\t f1[{}] \\t find attack [{}]\".format(max_conv, max_pre, max_recall, max_f1, 36 - max_zerolist))\n",
    "    return max_conv, max_pre, max_recall, max_f1, max_thresholds, precision, recall, f1, thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T05:45:23.915761Z",
     "start_time": "2019-04-16T05:45:23.882320Z"
    }
   },
   "outputs": [],
   "source": [
    "log = logging.getLogger('LSTM_log')\n",
    "log.setLevel(logging.DEBUG)\n",
    "\n",
    "formatter = logging.Formatter('%(asctime)s > %(message)s')\n",
    "\n",
    "fileHandler = logging.FileHandler('./log/logger.txt')\n",
    "\n",
    "fileHandler.setFormatter(formatter)\n",
    "log.addHandler(fileHandler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.selected_dim = [0, 40]\n",
    "solver = Solver(args = args)\n",
    "solver.fit(load = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T04:47:45.123777Z",
     "start_time": "2019-04-16T04:47:45.083113Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fin\n"
     ]
    }
   ],
   "source": [
    "candi_1 = [0, 1, 2, 3, 4,36,37,38,39,40]\n",
    "candi_2 = [0, 1, 2, 3, 4, 5]\n",
    "candi_3 = [35, 36,37,38,39,40]\n",
    "candi_4 = [17,18,19,20,21,22]\n",
    "\n",
    "log = logging.getLogger('LSTM_log')\n",
    "log.setLevel(logging.DEBUG)\n",
    "\n",
    "formatter = logging.Formatter('%(asctime)s > %(message)s')\n",
    "\n",
    "fileHandler = logging.FileHandler('./log/logger.txt')\n",
    "\n",
    "fileHandler.setFormatter(formatter)\n",
    "log.addHandler(fileHandler)\n",
    "# arg_list = []    \n",
    "# for i in range(len(candi_1)):\n",
    "#     arg_list.append(candi_1[i])  \n",
    "#     args.selected_dim = arg_list\n",
    "#     solver = Solver(args = args)\n",
    "#     solver.fit(load = False)\n",
    "#     arg_list = []   \n",
    "    \n",
    "arg_list = []    \n",
    "# for i in range(0,4):\n",
    "# #     for k in range(0,4):\n",
    "#     for p in range(0,4):\n",
    "#         arg_list.append(candi_2[i])\n",
    "#         arg_list.append(candi_2[i+1])\n",
    "#         arg_list.append(candi_2[i+2])\n",
    "#         arg_list.append(candi_3[k])\n",
    "#         arg_list.append( candi_3[k+1])\n",
    "#         arg_list.append( candi_3[k+2])\n",
    "#         arg_list.append(candi_4[p])\n",
    "#         arg_list.append(candi_4[p+1])\n",
    "#         arg_list.append(candi_4[p+2])\n",
    "#         args.selected_dim = arg_list\n",
    "#         solver = Solver(args = args)\n",
    "#         solver.fit(load = False)  \n",
    "#         arg_list = []   \n",
    "print(\"fin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T08:14:31.427585Z",
     "start_time": "2019-03-26T07:26:10.250661Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In train : {'Normal': 496800}\n",
      "In test : {'Attack': 54621, 'Normal': 395298}\n",
      "data length is 51\n"
     ]
    }
   ],
   "source": [
    "i = 3\n",
    "k =3 \n",
    "p= 3\n",
    "arg_list = [] \n",
    "arg_list.append(candi_2[i])\n",
    "arg_list.append(candi_2[i+1])\n",
    "arg_list.append(candi_2[i+2])\n",
    "arg_list.append(candi_3[k])\n",
    "arg_list.append( candi_3[k+1])\n",
    "arg_list.append( candi_3[k+2])\n",
    "arg_list.append(candi_4[p])\n",
    "arg_list.append(candi_4[p+1])\n",
    "arg_list.append(candi_4[p+2])\n",
    "args.selected_dim = arg_list\n",
    "solver = Solver(args = args)\n",
    "solver.fit(load = False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-23T05:23:00.327595Z",
     "start_time": "2019-03-23T04:44:21.313276Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In train : {'Normal': 496800}\n",
      "In test : {'Attack': 54621, 'Normal': 395298}\n",
      "data length is 51\n",
      "epoch[0]\t conv[190]\t precision[0.4968557953408604]\t recall[0.6364768129474012]\t f1[0.5580659919255805]\t findnum[25]\t time[232.773029088974]\n",
      "epoch[2]\t conv[190]\t precision[0.6496848335767546]\t recall[0.6189377711869062]\t f1[0.6339387006947506]\t findnum[23]\t time[694.000387430191]\n",
      "epoch[4]\t conv[180]\t precision[0.6844791962079164]\t recall[0.6186265355815529]\t f1[0.6498889284237452]\t findnum[21]\t time[1156.1872382164001]\n",
      "epoch[5]\t conv[190]\t precision[0.7115196842214605]\t recall[0.6270298969260907]\t f1[0.6666082759157616]\t findnum[21]\t time[1386.4979054927826]\n",
      "epoch[7]\t conv[190]\t precision[0.8244008427706083]\t recall[0.6877025319931894]\t f1[0.749872735439437]\t findnum[21]\t time[1848.0769476890564]\n",
      "epoch[8]\t conv[140]\t precision[0.7867742238063972]\t recall[0.7678182384064737]\t f1[0.7771806608231566]\t findnum[21]\t time[2078.4752616882324]\n"
     ]
    }
   ],
   "source": [
    "whole = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40]\n",
    "args.selected_dim = whole\n",
    "solver = Solver(args = args)\n",
    "solver.fit(load = False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
