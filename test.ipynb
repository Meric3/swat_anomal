{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T23:48:31.563638Z",
     "start_time": "2019-03-03T23:48:30.035843Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mjh319/anaconda3/envs/latest_3.6/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import utils.dataset as dataset\n",
    "import utils.preprocessing as preprocessing\n",
    "from utils.logger import Logger\n",
    "import lstm.model as model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T23:48:32.478380Z",
     "start_time": "2019-03-03T23:48:32.450733Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T23:49:07.957054Z",
     "start_time": "2019-03-03T23:49:07.932733Z"
    }
   },
   "outputs": [],
   "source": [
    "train_path= '../../DATA/SWaT/SWaT_Physical/SWaT_Dataset_Normal_v0.csv'\n",
    "test_path = '../../DATA/SWaT/SWaT_Physical/SWaT_Dataset_Attack_v0.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T23:49:14.984623Z",
     "start_time": "2019-03-03T23:49:08.895136Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mjh319/workspace/swat_anomal/utils/dataset.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  test_y[test_y == 'Attack'] = 1\n",
      "/home/mjh319/workspace/swat_anomal/utils/dataset.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  test_y[test_y == 'Normal'] = 0\n"
     ]
    }
   ],
   "source": [
    "train_x, test_x, test_y = dataset.pca_return(train_path = train_path, test_path = test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T06:02:06.675695Z",
     "start_time": "2019-02-28T06:02:06.658312Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T06:35:42.764950Z",
     "start_time": "2019-02-28T06:35:42.717765Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]predict 빈도: {-1: 4842, 1: 5158}\n"
     ]
    }
   ],
   "source": [
    "# comp_num = 45\n",
    "clf = svm.OneClassSVM(nu=0.001, kernel=\"rbf\", gamma=0.0001, verbose=True)\n",
    "clf.fit(train_x[0:20000,0:2])\n",
    "preds = clf.predict(test_x[5000:15000, 0:2]) \n",
    "unique, counts = np.unique(preds, return_counts=True)\n",
    "print('predict 빈도:', dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T06:10:27.161686Z",
     "start_time": "2019-02-28T06:10:27.112893Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]predict 빈도: {-1: 2390, 1: 7610}\n"
     ]
    }
   ],
   "source": [
    "# comp_num = 45\n",
    "clf = svm.OneClassSVM(nu=0.001, kernel=\"rbf\", gamma=0.0001, verbose=True)\n",
    "clf.fit(train_x[0:20000,0:4])\n",
    "preds = clf.predict(test_x[5000:15000, 0:4]) \n",
    "unique, counts = np.unique(preds, return_counts=True)\n",
    "print('predict 빈도:', dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T06:10:00.668563Z",
     "start_time": "2019-02-28T06:10:00.628819Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]predict 빈도: {-1: 4842, 1: 5158}\n"
     ]
    }
   ],
   "source": [
    "# comp_num = 2\n",
    "clf = svm.OneClassSVM(nu=0.001, kernel=\"rbf\", gamma=0.0001, verbose=True)\n",
    "clf.fit(train_x[0:20000,:])\n",
    "preds = clf.predict(test_x[5000:15000, :]) \n",
    "unique, counts = np.unique(preds, return_counts=True)\n",
    "print('predict 빈도:', dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T23:49:25.065766Z",
     "start_time": "2019-03-03T23:49:25.032434Z"
    }
   },
   "outputs": [],
   "source": [
    "class ENCODER(nn.Module):\n",
    "\n",
    "    def __init__(self,args):\n",
    "        super(ENCODER, self).__init__()\n",
    "        self.args = args\n",
    "        self.drop = nn.Dropout(args['dropout'])\n",
    "        self.linear = nn.Linear(args['hidden_size'], args['data_dim'])\n",
    "\n",
    "        if args['cell_type'] in ['LSTM', 'GRU']:\n",
    "            self.rnn = getattr(nn, args['cell_type'])(args['rnn_inp_size'], args['hidden_size'], args['nlayers'], dropout=args['dropout'])\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
    "        self.linear.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, input, hidden, return_hiddens=False, noise=False):\n",
    "        output, hidden = self.rnn(input, hidden)\n",
    "        output = self.linear(output.contiguous().view(-1,self.args['hidden_size']))\n",
    "        output = output.contiguous().view(input.size()[0], -1, self.args['rnn_inp_size'])\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters()).data ############# 이게 무엇\n",
    "        if self.args['cell_type'] == 'LSTM':\n",
    "            return (Variable(weight.new(self.args['nlayers'], bsz, self.args['hidden_size']).zero_()),\n",
    "                    Variable(weight.new(self.args['nlayers'], bsz, self.args['hidden_size']).zero_()))\n",
    "\n",
    "    def repackage_hidden(self,h):\n",
    "        \"\"\"Wraps hidden states in new Variables, to detach them from their history.\"\"\"\n",
    "        if type(h) == tuple:\n",
    "            return tuple(self.repackage_hidden(v) for v in h)\n",
    "        else:\n",
    "            return Variable(h.data)\n",
    "\n",
    "    def extract_hidden(self, hidden):\n",
    "        if self.args['cell_type'] == 'LSTM':\n",
    "            return hidden[0][-1].data.cpu()  # hidden state last layer (hidden[1] is cell state)\n",
    "        else:\n",
    "            return hidden[-1].data.cpu()  # last layer\n",
    "\n",
    "        \n",
    "class DECODER(nn.Module):\n",
    "\n",
    "    def __init__(self,args):\n",
    "        super(DECODER, self).__init__()\n",
    "        self.args = args\n",
    "        self.drop = nn.Dropout(args['dropout'])\n",
    "        self.linear = nn.Linear(args['hidden_size'], args['data_dim'])\n",
    "\n",
    "        if args['cell_type'] in ['LSTM', 'GRU']:\n",
    "            self.rnn = getattr(nn, args['cell_type'])(args['rnn_inp_size'], args['hidden_size'], args['nlayers'], dropout=args['dropout'])\n",
    "\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
    "        self.linear.bias.data.fill_(0)\n",
    "        \n",
    "    def forward(self, input, hidden, return_hiddens=False, noise=False):\n",
    "        \n",
    "        output, hidden = self.rnn(input, hidden)\n",
    "        output = self.linear(output.contiguous().view(-1,self.args['hidden_size']))\n",
    "        output = output.contiguous().view(input.size()[0], -1, self.args['rnn_inp_size'])\n",
    "\n",
    "        return output, hidden\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters()).data ############# 이게 무엇\n",
    "        if self.args['cell_type'] == 'LSTM':\n",
    "            return (Variable(weight.new(self.args['nlayers'], bsz, self.args['hidden_size']).zero_()),\n",
    "                    Variable(weight.new(self.args['nlayers'], bsz, self.args['hidden_size']).zero_()))\n",
    "\n",
    "    def repackage_hidden(self,h):\n",
    "        \"\"\"Wraps hidden states in new Variables, to detach them from their history.\"\"\"\n",
    "        if type(h) == tuple:\n",
    "            return tuple(self.repackage_hidden(v) for v in h)\n",
    "        else:\n",
    "            return Variable(h.data)\n",
    "\n",
    "    def extract_hidden(self, hidden):\n",
    "        if self.args['cell_type'] == 'LSTM':\n",
    "            return hidden[0][-1].data.cpu()  # hidden state last layer (hidden[1] is cell state)\n",
    "        else:\n",
    "            return hidden[-1].data.cpu()  # last layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T23:51:55.164168Z",
     "start_time": "2019-03-03T23:51:55.131704Z"
    }
   },
   "outputs": [],
   "source": [
    "class Solver():\n",
    "    def __init__(self, args):\n",
    "        self.train_x_batchfy =args['train_x_batchfy']\n",
    "        self.test_x_batchfy =args['test_x_batchfy']       \n",
    "        self.generate_batchfy = args['generate_batchfy'] \n",
    "        self.train_generate_batchfy = args['train_generate_batchfy']\n",
    "        self.test_y = args['test_y']\n",
    "        \n",
    "\n",
    "        self.args = args\n",
    "        self.encoder = model.ENCODER(self.args)\n",
    "        self.encoder.cuda()\n",
    "\n",
    "        self.decoder = model.DECODER(self.args)\n",
    "        self.decoder.cuda()\n",
    "\n",
    "        self.optim_enc   = torch.optim.Adam(self.encoder.parameters(), self.args['lr'])\n",
    "        self.optim_dec   = torch.optim.Adam(self.decoder.parameters(), self.args['lr'])\n",
    "\n",
    "        self.loss_fn = nn.MSELoss()    \n",
    "    \n",
    "        self.logger = Logger('./tf_logs')\n",
    "    \n",
    "        self.base_dir = Path('save',self.make_dir_name(args))\n",
    "        self.base_dir.mkdir(parents=True,exist_ok=True)      \n",
    "    \n",
    "    def make_dir_name(self, args):\n",
    "        return 'modelName:'+args['model_name']+'__cellType:'+args['cell_type'] \\\n",
    "                + '__hidSize:' + str(args['hidden_size']) + '__dropout:' + str(args['dropout'])\n",
    "\n",
    "    def load(self, path):\n",
    "        try:\n",
    "            checkpoint = torch.load(Path(path))\n",
    "            start_epoch = checkpoint['epoch']\n",
    "            self.encoder.load_state_dict(checkpoint['state_dict_enc'])\n",
    "            self.optim_enc.load_state_dict((checkpoint['optimizer_enc']))\n",
    "            self.decoder.load_state_dict(checkpoint['state_dict_dec'])\n",
    "            self.optim_dec.load_state_dict((checkpoint['optimizer_dec']))            \n",
    "            del checkpoint\n",
    "            print(\"=> loaded checkpoint\")\n",
    "        except:\n",
    "            print(\"=> Not exist checkpoint\")\n",
    "            pass        \n",
    "\n",
    "    def fit(self, load):\n",
    "        if load == True:\n",
    "            try:\n",
    "                checkpoint = torch.load(Path(self.base_dir,'checkpoint').with_suffix('.pth'))\n",
    "                start_epoch = checkpoint['epoch']\n",
    "                self.encoder.load_state_dict(checkpoint['state_dict_enc'])\n",
    "                self.optim_enc.load_state_dict((checkpoint['optimizer_enc']))\n",
    "                self.decoder.load_state_dict(checkpoint['state_dict_dec'])\n",
    "                self.optim_dec.load_state_dict((checkpoint['optimizer_dec']))            \n",
    "                del checkpoint\n",
    "                print(\"=> loaded checkpoint\")\n",
    "            except:\n",
    "                print(\"=> Not exist checkpoint\")\n",
    "                pass\n",
    "\n",
    "        total_loss = 0\n",
    "\n",
    "        for epoch in range(0, self.args['epoch']):\n",
    "\n",
    "            self.encoder.train()\n",
    "            self.decoder.train()\n",
    "            \n",
    "            hidden_enc = self.encoder.init_hidden(self.args['batch_size'])\n",
    "\n",
    "            for batch, i in enumerate(range(0, self.train_x_batchfy.size(0) - 1, self.args['seq_length'])):\n",
    "                outSeq = []\n",
    "                inputSeq, targetSeq = preprocessing.get_batch(self.args, self.train_x_batchfy, i)\n",
    "\n",
    "                if args['seq_length'] != targetSeq.size()[0] :\n",
    "                    continue\n",
    "                hidden_enc = self.encoder.repackage_hidden(hidden_enc)\n",
    "                self.optim_enc.zero_grad()\n",
    "                self.optim_dec.zero_grad()\n",
    "                \n",
    "                Outputseq_enc, hidden_enc = self.encoder.forward(inputSeq, hidden_enc, return_hiddens=True)\n",
    "                deccoder_input = Variable(torch.zeros(Outputseq_enc.size())).cuda()\n",
    "                \n",
    "                deccoder_input[0,:,:] = Outputseq_enc[-1,:,:]\n",
    "                deccoder_input[1:,:,:] = targetSeq[:-1,:,:]\n",
    "                \n",
    "                loss_enc = self.loss_fn(Outputseq_enc[-1,:,:].view(self.args['batch_size'], -1), targetSeq[0,:,:].contiguous().view(self.args['batch_size'], -1))\n",
    "                loss_enc.backward(retain_graph=True)\n",
    "                \n",
    "                total_norm_1 = max(p.grad.data.abs().max() for p in self.encoder.parameters())\n",
    "                \n",
    "                torch.nn.utils.clip_grad_norm(self.encoder.parameters(), 0.1)\n",
    "                \n",
    "                total_norm_2 = max(p.grad.data.abs().max() for p in self.encoder.parameters())\n",
    "#                 break\n",
    "                \n",
    "#                 print(\"parameter norm {}, after clip {}\".format(total_norm_1, total_norm_2))\n",
    "                print(batch, self.train_x_batchfy.size(0) - 1)\n",
    "                if batch > 10:\n",
    "                    break\n",
    "                \n",
    "                self.optim_enc.step()     \n",
    "                \n",
    "                Outputseq_enc, hidden_enc = self.decoder.forward(deccoder_input, hidden_enc, return_hiddens=True)\n",
    "                loss_dec = self.loss_fn(Outputseq_enc.view(args['batch_size'], -1), targetSeq.contiguous().view(args['batch_size'], -1))   \n",
    "                loss_dec.backward()\n",
    "                torch.nn.utils.clip_grad_norm(self.decoder.parameters(), self.args['clip'])\n",
    "                self.optim_dec.step()\n",
    "                \n",
    "                total_loss += loss_enc.item() + loss_dec.item()        \n",
    "\n",
    "                if batch % 10 == 0 :\n",
    "                    # 1. Log scalar values (scalar summary)\n",
    "                    info = { 'enc_loss': loss_enc.item(), 'dec_loss' : loss_dec.item() }\n",
    "\n",
    "#                     for tag, value in info.items():\n",
    "#                         self.logger.scalar_summary(tag, value, step+1)\n",
    "\n",
    "#                     # 2. Log values and gradients of the parameters (histogram summary)\n",
    "#                     for tag, value in self.net.named_parameters():\n",
    "#                         tag = tag.replace('.', '/')\n",
    "#                         self.logger.histo_summary(tag, value.data.cpu().numpy(), step+1)\n",
    "              \n",
    "            \n",
    "#             self.model_dictionary = {'epoch': epoch + 1,\n",
    "#                     'state_dict_enc': self.encoder.state_dict(),\n",
    "#                     'optimizer_enc': self.optim_enc.state_dict(),\n",
    "#                     'state_dict_dec': self.decoder.state_dict(),\n",
    "#                     'optimizer_dec': self.optim_dec.state_dict(),\n",
    "#                     'args':args,\n",
    "#                     'loss':total_loss\n",
    "#                     }\n",
    "#             self.save_checkpoint(self.args, self.model_dictionary)\n",
    "            total_loss = 0\n",
    "\n",
    " \n",
    "    def save_checkpoint(self, args, state):\n",
    "        checkpoint = Path(self.base_dir, 'checkpoint')\n",
    "        checkpoint = checkpoint.with_suffix('.pth')\n",
    "        torch.save(state, checkpoint)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T23:49:39.273136Z",
     "start_time": "2019-03-03T23:49:39.247919Z"
    }
   },
   "outputs": [],
   "source": [
    "args={}\n",
    "args['model_name'] = 'enc_dec'\n",
    "args['cuda'] = True\n",
    "args['batch_size'] = 256\n",
    "args['clip'] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T23:49:40.257852Z",
     "start_time": "2019-03-03T23:49:40.233198Z"
    }
   },
   "outputs": [],
   "source": [
    "import utils.preprocessing as preprocessing\n",
    "import utils.postprocessing as postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T23:49:47.638583Z",
     "start_time": "2019-03-03T23:49:45.568320Z"
    }
   },
   "outputs": [],
   "source": [
    "train_x_batchfy = preprocessing.batchify(args, train_x, args['batch_size'])\n",
    "test_x_batchfy = preprocessing.batchify(args, test_x, args['batch_size'])\n",
    "generate_batchfy = preprocessing.batchify(args, test_x, 1)\n",
    "train_generate_batchfy = preprocessing.batchify(args, train_x, 1)\n",
    "\n",
    "args['test_y'] = test_y\n",
    "args['cell_type'] = 'LSTM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T23:49:50.995099Z",
     "start_time": "2019-03-03T23:49:50.964633Z"
    }
   },
   "outputs": [],
   "source": [
    "train_col = 3\n",
    "args['epoch'] = 1\n",
    "args['seq_length'] = 2\n",
    "args['hidden_size'] = 32\n",
    "args['train_x_batchfy'] = train_x_batchfy[:,:,[0,1,2]]\n",
    "args['test_x_batchfy'] = test_x_batchfy[:,:,[0,1,2]]\n",
    "args['generate_batchfy'] =generate_batchfy[:,:,[0,1,2]]\n",
    "args['train_generate_batchfy'] =train_generate_batchfy[:,:,[0,1,2]]\n",
    "args['data_dim'] = train_col\n",
    "args['dropout'] = 0.5\n",
    "args['nlayers'] = 2\n",
    "args['rnn_inp_size'] = args['data_dim']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T23:52:04.800839Z",
     "start_time": "2019-03-03T23:52:04.645091Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1939\n",
      "1 1939\n",
      "2 1939\n",
      "3 1939\n",
      "4 1939\n",
      "5 1939\n",
      "6 1939\n",
      "7 1939\n",
      "8 1939\n",
      "9 1939\n",
      "10 1939\n",
      "11 1939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mjh319/anaconda3/envs/latest_3.6/lib/python3.6/site-packages/ipykernel_launcher.py:89: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "/home/mjh319/anaconda3/envs/latest_3.6/lib/python3.6/site-packages/ipykernel_launcher.py:104: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
     ]
    }
   ],
   "source": [
    "args['lr'] =0.001\n",
    "solver = Solver(args = args)\n",
    "solver.fit(load = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T12:06:03.810464Z",
     "start_time": "2019-03-03T12:02:48.594505Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anomal_detection...\n"
     ]
    }
   ],
   "source": [
    "import utils.postprocessing as postprocessing\n",
    "anomal_score = postprocessing.get_anomalscore_encdec(base_model = solver,  generate_batchfy = solver.generate_batchfy, length = 449916,args = args)\n",
    "# _,_,_ = utils.evaluate_conv(nomalize_scores = anomal_score ,num_samples = 100000, conv = 0, check_step = 1000, attack_list = attack_list, length = 449916,prints=True,args = args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T12:09:47.988586Z",
     "start_time": "2019-03-03T12:09:47.968236Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(449916, 3)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anomal_score.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T12:10:02.211696Z",
     "start_time": "2019-03-03T12:10:02.192808Z"
    }
   },
   "outputs": [],
   "source": [
    "import logging as log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T12:11:21.750379Z",
     "start_time": "2019-03-03T12:11:21.730365Z"
    }
   },
   "outputs": [],
   "source": [
    "from logging import handlers\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T12:10:20.975151Z",
     "start_time": "2019-03-03T12:10:20.953489Z"
    }
   },
   "outputs": [],
   "source": [
    "log.basicConfig(filename='./log.txt', level=log.DEBUG)\n",
    "\n",
    "log.debug('debug')\n",
    "log.info('info')\n",
    "log.warning('warning')\n",
    "log.error('error')\n",
    "log.critical('critical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T12:16:39.020728Z",
     "start_time": "2019-03-03T12:16:38.997047Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Logger' object has no attribute 'Formatter'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-122-200c2bd0bb5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcarLogFormatter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFormatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%(asctime)s,%(message)s'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Logger' object has no attribute 'Formatter'"
     ]
    }
   ],
   "source": [
    "carLogFormatter = log.Formatter('%(asctime)s,%(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T12:13:38.064950Z",
     "start_time": "2019-03-03T12:13:38.044125Z"
    }
   },
   "outputs": [],
   "source": [
    "#handler settings\n",
    "carLogHandler = handlers.TimedRotatingFileHandler(filename='car.log', interval=1, encoding='utf-8')\n",
    "carLogHandler.setFormatter(carLogFormatter)\n",
    "# carLogHandler.suffix = \"%Y%m%d\"\n",
    "\n",
    "#logger set\n",
    "carLogger = logging.getLogger()\n",
    "carLogger.setLevel(logging.INFO)\n",
    "carLogger.addHandler(carLogHandler)\n",
    "\n",
    "#use logger\n",
    "carLogger.info(\"car is coming22\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T12:16:46.363967Z",
     "start_time": "2019-03-03T12:16:46.342163Z"
    }
   },
   "outputs": [],
   "source": [
    "carLogFormatter = logging.Formatter('%(asctime)s,%(message)s')\n",
    "log = logging.getLogger('snowdeer_log')\n",
    "log.setLevel(logging.DEBUG)\n",
    "\n",
    "fileHandler = logging.FileHandler('./log.txt')\n",
    "fileHandler.setFormatter(carLogFormatter)\n",
    "streamHandler = logging.StreamHandler()\n",
    "\n",
    "\n",
    "log.addHandler(fileHandler)\n",
    "log.addHandler(streamHandler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T12:16:49.596393Z",
     "start_time": "2019-03-03T12:16:49.573324Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "info\n",
      "info\n"
     ]
    }
   ],
   "source": [
    "log.info('info')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
